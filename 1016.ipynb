{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBQ0EtA0sQKER1/fQoXqLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1016.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7mjDKvxX8IU"
      },
      "outputs": [],
      "source": [
        "TextMining - Information Retrieval\n",
        "User's information needs 어떻게?\n",
        "Filterling vs. Ranking\n",
        "-> Boolean; True or False => 부분집합;Query 잘 작성;모든사람X;연관성(관련성)X\n",
        "-> Ranking; Relevance높음=Similarity높음; Top-K; Query 고민 X\n",
        "Similarity     Probabilities(Okapi;BM25)      Inference(NN)X\n",
        "                    TF-IDF변형\n",
        "VectorSpace Model\n",
        "V = {t1,t2,..., tm} ; Controlled Vocabulary\n",
        "Q = {w1, w2, ..., wn ㅌ V} ; Short Document\n",
        "C = {d1, d2, ..., dk}\n",
        "di = {wi1, wi2, ..., win ㅌ V}\n",
        "Rel(Q, D) = Sim(Q,D) => Sim(Rep(Q),Rep(D))\n",
        "\n",
        "Inverted Index(g/dbml => FileSystem;B-tree) => Special structure\n",
        "; A = np.zeros((100000000,100000000))\n",
        "; A[0,:] => one vector * A => 터짐\n",
        "X => O(|Q|*D*|D|) => DTM\n",
        "O => O(|Q|*|L|) => TDM\n",
        "        V  => Posting(LinkedList-File; Data append; on-disk)\n",
        "     Vocabulary(Heaps;in-memory) => Feature\n",
        "Dictionary(K:V-Posting=>File)\n",
        "IndexTable(색인표) => D1:A    ....     12\n",
        "                    D2:A ...\n",
        "\n",
        "                    T1: (D1,D2 ...) => File LinkedList\n",
        "                    T2: ...\n",
        "; Single Machine Processing\n",
        "Map-Reduce => GFS, Hadoop => Spark => BigData\n",
        "(K,V), (합치는)\n",
        "=> Local, Global Merge\n",
        "%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202023-10-16%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2010.28.27.png\n",
        "\n",
        "from math import log\n",
        "\n",
        "tf1 = lambda tf:1 if tf > 0 else 0\n",
        "tf2 = lambda tf:tf\n",
        "tf3 = lambda tf, ttf:tf/ttf\n",
        "tf4 = lambda tf:log(1+tf)\n",
        "tf5 = lambda tf, mtf, K=0.5:K+(1-K)*(tf/mtf)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TF = [10, 20, 30, 40, 50]\n",
        "TTF = sum(TF)\n",
        "MTF = max(TF)\n",
        "plt.plot(TF, list(map(tf1, TF)), 'r-')\n",
        "# plt.plot(TF, list(map(tf2, TF)), 'g-')\n",
        "plt.plot(TF, list(map(tf3, TF, [TTF]*len(TF))), 'b-')\n",
        "# plt.plot(TF, list(map(tf4, TF)), 'c-')\n",
        "plt.plot(TF, list(map(tf5, TF, [MTF]*len(TF))), 'm-')\n",
        "plt.plot(TF, list(map(tf5, TF, [MTF]*len(TF), [0]*len(TF))), 'y-')\n",
        "[<matplotlib.lines.Line2D at 0x11adc8e50>]\n",
        "\n",
        "image.png\n",
        "\n",
        "idf1 = lambda df:1\n",
        "idf2 = lambda df, N:log(N/df)\n",
        "idf3 = lambda df, N:log(N/(1+df))+1\n",
        "idf4 = lambda df, mdf:log(mdf/(1+df))\n",
        "idf5 = lambda df, N:log((N-df)/df)\n",
        "DF = [10, 20, 30, 40, 50]\n",
        "N = max(DF)+1\n",
        "MDF = max(DF)\n",
        "\n",
        "plt.plot(DF, list(map(idf1, DF)), 'r-')\n",
        "plt.plot(DF, list(map(idf2, DF, [N]*len(DF))), 'g-')\n",
        "plt.plot(DF, list(map(idf3, DF, [N]*len(DF))), 'b-')\n",
        "plt.plot(DF, list(map(idf4, DF, [MDF]*len(DF))), 'c-')\n",
        "# plt.plot(DF, list(map(idf5, DF, [N]*len(DF))), 'm-')\n",
        "[<matplotlib.lines.Line2D at 0x11f9b7dc0>]\n",
        "\n",
        "       +   *\n",
        "       |  / (1사분면)\n",
        "       | /\n",
        "       |/         Controlled Voca. => Token(1개 이상)\n",
        "-------0---+----- Concept=Dimension=Orthogonal=Independent=BoW\n",
        "      /|   weight=importance / each dimension\n",
        "     / |\n",
        "    *  |\n",
        "       |\n",
        "# [고빈도]\n",
        "# 1. TF 높은데, DF 높은\n",
        "# 2.          DF 낮은   ***\n",
        "# [중빈도]\n",
        "# 3. TF 평균, DF 평균\n",
        "# [저빈도]\n",
        "# 4. TF 낮은데, DF 높은\n",
        "# 5.         DF 낮은 ***\n",
        "TF = [50, 50, 25, 10, 10]\n",
        "DF = [50, 10, 25, 50, 10]\n",
        "\n",
        "X = list(zip(TF, DF))\n",
        "# 5(k=0), 5(k=.5)\n",
        "# 2\n",
        "\n",
        "plt.plot(range(len(TF)), [tf5(tf, 50, 0)*idf1(df)\n",
        "                          for tf, df in X], 'r-')\n",
        "plt.plot(range(len(TF)), [tf5(tf, 50, 0)*idf2(df, 50)\n",
        "                          for tf, df in X], 'g-')\n",
        "plt.plot(range(len(TF)), [tf5(tf, 50, 0)*idf3(df, 50)\n",
        "                          for tf, df in X], 'b-')\n",
        "plt.plot(range(len(TF)), [tf5(tf, 50, 0)*idf4(df, 50)\n",
        "                          for tf, df in X], 'k-')\n",
        "[<matplotlib.lines.Line2D at 0x11f9fe5e0>]\n",
        "\n",
        "Dictionary + Posting\n",
        "{term:파일위치} [124][123][123]124123123123\n",
        "              어느문서, 몇번나왔다=freq, 다음위치\n",
        "                      가중치=weight, 다음위치\n",
        "\n",
        "TF5: freq(포스팅 있음), maxfreq(있긴 한데, 다 뒤져야 함), K(우리가 정함)\n",
        "IDF2: docFreq(-1까지 몇번 돌았는지), N(len(Collection))\n",
        "\n",
        "t1 = {d1, d2, ..., dn}\n",
        "      0   10        0 => Sparse to Dense\n",
        "           w* => TF-IDF\n",
        "t2 = {         }\n",
        "d2\n",
        "|   t1\n",
        "|  /  t2\n",
        "| /  /\n",
        "|/  /\n",
        "------------ d1\n",
        "import re\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from os import listdir\n",
        "from struct import pack, unpack\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(''.join(s[i:i+n]))\n",
        "    return rst\n",
        "\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "\n",
        "p1 = re.compile(r'\\s+')\n",
        "\n",
        "D = list() # {file:문서1, maxfreq:뭐고}, 문서, ...\n",
        "V = list()\n",
        "TDM = dict() # {term1:[위치, df]}\n",
        "\n",
        "Posting = open('posting.dat', 'wb')\n",
        "Posting.close()\n",
        "\n",
        "for file in fileids('naver'):\n",
        "    i = len(D)\n",
        "    D.append({'filename':file, 'maxfreq':0})\n",
        "\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        corpus = fp.read()\n",
        "\n",
        "    localTDM = dict()\n",
        "    localPosting = open('local.dat', 'wb')\n",
        "\n",
        "    # Tokenizing+Normalizing\n",
        "    for t in regexp_tokenize(p1.sub(' ', corpus), r'\\b\\w+\\b'):\n",
        "        for g in ngram(t):\n",
        "            # 문서 1개 작업하는 중에, 단어가 처음 등장\n",
        "            if g not in localTDM.keys():\n",
        "                pos = localPosting.tell()\n",
        "                localPosting.write(pack('ii', 1, -1))\n",
        "                localTDM[g] = pos\n",
        "            else:\n",
        "                pos = localPosting.tell()\n",
        "                localPosting.write(pack('ii', 1, localTDM[g]))\n",
        "                localTDM[g] = pos\n",
        "\n",
        "    localPosting.close()\n",
        "\n",
        "    # Update; Local -> Global\n",
        "    Posting = open('posting.dat', 'ab')\n",
        "    localPosting = open('local.dat', 'rb')\n",
        "\n",
        "    # k=단어, v=파일위치\n",
        "    maxFreq = 0\n",
        "    for k, v in localTDM.items():\n",
        "        if k not in V:\n",
        "            V.append(k)\n",
        "\n",
        "        j = V.index(k)\n",
        "\n",
        "        freq = 0\n",
        "        pos = v\n",
        "        while pos != -1:\n",
        "            localPosting.seek(pos)\n",
        "            f, npos = unpack('ii', localPosting.read(8))\n",
        "            pos = npos\n",
        "            freq += 1\n",
        "\n",
        "        if freq > maxFreq:\n",
        "            maxFreq = freq\n",
        "\n",
        "        if j not in TDM.keys():\n",
        "            pos = Posting.tell()\n",
        "            Posting.write(pack('iii', i, freq, -1))\n",
        "            TDM[j] = {'fp':pos, 'df':1}\n",
        "        else:\n",
        "            pos = Posting.tell()\n",
        "            Posting.write(pack('iii', i, freq, TDM[j]['fp']))\n",
        "            TDM[j]['fp'] = pos\n",
        "            TDM[j]['df'] += 1\n",
        "\n",
        "    D[i]['maxfreq'] = maxFreq\n",
        "\n",
        "    localPosting.close()\n",
        "    Posting.close()\n",
        "i2w = lambda i:V[i]\n",
        "w2i = lambda w:V.index(w)\n",
        "\n",
        "i2d = lambda i:D[i]\n",
        "d2i = lambda d:D.index(d)\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "result = list()\n",
        "\n",
        "Posting = open('posting.dat', 'rb')\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            result.append(list())\n",
        "\n",
        "            j = w2i(g)\n",
        "            pos = TDM[j]['fp']\n",
        "            while pos != -1:\n",
        "                Posting.seek(pos)\n",
        "                i, freq, npos = unpack('iii', Posting.read(12))\n",
        "                pos = npos\n",
        "\n",
        "                result[-1].append(i)\n",
        "            result[-1] = list(set(result[-1]))\n",
        "\n",
        "Posting.close()\n",
        "WDM = dict()\n",
        "Posting = open('posting.dat', 'rb')\n",
        "Weighting = open('weighting.dat', 'wb')\n",
        "\n",
        "for k, v in TDM.items():\n",
        "    fp = Weighting.tell()\n",
        "    WDM[k] = {'fp':fp, 'df':v['df']}\n",
        "\n",
        "    pos = v['fp']\n",
        "    while pos != -1:\n",
        "        Posting.seek(pos)\n",
        "        i, freq, npos = unpack('iii', Posting.read(12))\n",
        "\n",
        "        tf = tf5(freq, D[i]['maxfreq'], 0)\n",
        "        idf = idf2(v['df'], len(D))\n",
        "        w = tf*idf\n",
        "        Weighting.write(pack('if', i, w))\n",
        "\n",
        "        pos = npos\n",
        "\n",
        "Posting.close()\n",
        "Weighting.close()\n",
        "Posting\n",
        "어느문서,몇번,다음위치*\n",
        "\n",
        "df=3\n",
        "Weighting\n",
        "fp:어느문서,가중치,어느문서,가중치,어느문서,가중치 => Sequential\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "Weighting = open('weighting.dat', 'rb')\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            j = w2i(g)\n",
        "\n",
        "            n = 0\n",
        "            while n < WDM[j]['df']:\n",
        "                Weighting.seek(WDM[j]['fp']+n*8)\n",
        "                i, w = unpack('if', Weighting.read(8))\n",
        "                pos = npos\n",
        "                n += 1\n",
        "\n",
        "                print(f'{i}번째 문서, {j}번째 단어 {g}의 가중치 {w}')\n",
        "\n",
        "Weighting.close()"
      ]
    }
  ]
}