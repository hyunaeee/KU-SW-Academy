{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC2mgvCViBsHZzTMHUzmfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEi8BHDAXLBZ"
      },
      "outputs": [],
      "source": [
        "Tokenizing\n",
        "[Feature Extraction]\n",
        "- sent_tokenize; ë¬¸ì¥(êµ¬ë‘ì ) => ë¬¸ë‹¨ => ë¬¸ì„œ\n",
        "- word_tokenize; regex; tweet; term/token/word/subword ...\n",
        "                 => ìš©ì–´ => ì–´íœ˜ì§‘í•©(Voca)\n",
        "                 => Text.vocab() = FreqDist = Count\n",
        "[Featrue Selection]\n",
        "=> Zipf; ìì—°ì–´, ìˆœìœ„ì˜ ì—­ê³¼ ë¹ˆë„ë¡œ ë‚˜ì—´í–ˆì„ë•Œ ê°™ë”ë¼, ì „ ìˆœìœ„ë³´ë‹¤ 2ë°° ê°€ëŸ‰ ë§ì´ ë‚˜ì˜´\n",
        "       ; ê³ ë¹ˆë„(50%, ìƒìœ„ ì†Œìˆ˜ì˜ ë‹¨ì–´) - ì¤‘ë¹ˆë„(*) - ì €ë¹ˆë„(25%, í¬ì†Œí•œ)\n",
        "         => TF-IDF, PRP->Okapi->BM25 => TF-IDF\n",
        "=> Heaps; corpusê°€ ì»¤ì§ˆìˆ˜ë¡ unique ë‹¨ì–´ë“¤ì´ (íŠ¹ì • ë²•ì¹™) ì¦ê°€\n",
        "        ; k,b=>.4~.6 => feature ìˆ˜ë¥¼ ì˜ˆì¸¡ / Zipf ì–‘ì§ˆì˜ feature\n",
        "\n",
        "[Korean]\n",
        "- Morpheme/POS\n",
        "  -> í˜•íƒœì†Œ/í˜•íƒœì†Œ/í˜•íƒœì†Œ\n",
        "  -> NN    VV   EX\n",
        "- Grammar -> Rules => RNN\n",
        "  -> NP      VP  => S\n",
        "=> N-gram(Nê°œì˜ gram=token) => LM\n",
        "   1(uni), 2(bi), 3(tri), 4(...), ... (X)\n",
        "ğ‘·(ğ’™ğ’Š|ğ’™ğ’Šâˆ’ ğ’âˆ’ğŸ ,...,ğ’™ğ’Šâˆ’ğŸ) => N-gram P(A,B) = ? A=ë°°, B=ê³ í”„ë‹¤, ê²½ìš°ì˜ ìˆ˜ë¥¼ ëª¨ë‘ ì¡°í•© => X P(A,B) => P(B|A)P(A) P(B|A) => P(A,B)/P(A) ------ ---- bigram uigram ë°°+?; MLE P(freq(A+B)/freq(A)) N N (ë‹¨ì–´,ë‹¨ì–´), (ë‹¨ì–´,ë‹¨ì–´) => LM P(C|A,B) => P(A,B,C)/P(A,B) => (ë‹¨ì–´,ë‹¨ì–´,ë‹¨ì–´) P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... => 0 Markov Assumption(1st, 2nd, ..) P(A,B,C) => P(C|A,B)P(B|A)P(A) ~= P(B,C) P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... ~= P(Z|Y)P(Y|X)P(X|..)... 1st Markov Assumption => Bigram 2nd Markov Assumption => Trigram P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... ~= P(Z|XY)P(Y|WX)P(X|VW)... Z|XA, XB, XC, .., XY ABC ABD=0 ABE ACA ACG ACE=0 ... => ì•„ ___ => Sequence(L;given->R) => RNN;LSTM;GRU / Attention -----> <----- A B C [mask] E F G; LM? => Auto-encoder - - - - P(A) P(B|A) ... A B C D E F ...\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(tuple(s[i:i+n]))\n",
        "    return rst\n",
        "from konlpy.corpus import kolaw\n",
        "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
        "from konlpy.tag import Komoran\n",
        "ma = Komoran()\n",
        "Python        JPype        Java(JVM)\n",
        "Komoran() --------------> Komoran()\n",
        "id        <--------------\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "tokens = word_tokenize(corpus)\n",
        "bigram = ngram(tokens)\n",
        "biText = Text(bigram)\n",
        "unigram = ngram(tokens, 1)\n",
        "len(list(filter(lambda t:t[0] == 'ë²•ë¥ ì´', unigram)))\n",
        "57\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "a = FreqDist(list(filter(lambda t:t[0] == 'ë²•ë¥ ì´', bigram)))\n",
        "for k, v in a.items():\n",
        "    print(f'{k}-{v} => P({k}|ë²•ë¥ ì´)={v/sum(a.values())}')\n",
        "('ë²•ë¥ ì´', 'ì •í•˜ëŠ”')-48 => P(('ë²•ë¥ ì´', 'ì •í•˜ëŠ”')|ë²•ë¥ ì´)=0.8421052631578947\n",
        "('ë²•ë¥ ì´', 'ì •í•œ')-7 => P(('ë²•ë¥ ì´', 'ì •í•œ')|ë²•ë¥ ì´)=0.12280701754385964\n",
        "('ë²•ë¥ ì´', 'í™•ì •ëœ')-1 => P(('ë²•ë¥ ì´', 'í™•ì •ëœ')|ë²•ë¥ ì´)=0.017543859649122806\n",
        "('ë²•ë¥ ì´', 'í—Œë²•ì—')-1 => P(('ë²•ë¥ ì´', 'í—Œë²•ì—')|ë²•ë¥ ì´)=0.017543859649122806\n",
        "# ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - Crawler\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "urls = list()\n",
        "seens = list()\n",
        "\n",
        "path = 'naver/'\n",
        "urls.append('https://news.naver.com/')\n",
        "\n",
        "headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}\n",
        "\n",
        "while urls:\n",
        "    url = urls.pop(0)\n",
        "    seens.append(url)\n",
        "\n",
        "    dom = BeautifulSoup(get(url,headers=headers).text, 'html.parser')\n",
        "\n",
        "    # ë©”ë‰´\n",
        "    alist = dom.select('ul[role=menu] > li > a[href]')\n",
        "    for a in alist:\n",
        "        if re.search(r'sid1=\\d{3}$', a.attrs['href']):\n",
        "            if a.attrs['href'] not in urls and\\\n",
        "               a.attrs['href'] not in seens:\n",
        "                urls.append(a.attrs['href'])\n",
        "\n",
        "    # ê¸°ì‚¬ë§í¬ => ì •êµí•˜ê²Œ\n",
        "    alist = dom.select('''\n",
        "                        .sh_text > a[href],\n",
        "                        .cluster_text > a[href],\n",
        "                        dt > a[href]\n",
        "                    ''')\n",
        "    for a in alist:\n",
        "        if a.attrs['href'] not in urls and\\\n",
        "           a.attrs['href'] not in seens:\n",
        "            urls.append(a.attrs['href'])\n",
        "\n",
        "    # ë³¸ë¬¸\n",
        "    news = dom.select_one('#newsct_article')\n",
        "    if news:\n",
        "        file = re.search(r'(\\d{10})', url).group(1)\n",
        "        with open(f'{path}{file}.txt', 'w', encoding='utf-8') as fp:\n",
        "            fp.write(news.text)\n",
        "from os import listdir, mkdir\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "corpus = list()\n",
        "for f in fileids('naver'):\n",
        "    with open(f, 'r', ecoding='utf8') as fp:\n",
        "        corpus.append(fp.read())\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "wtText = list()\n",
        "maText = list()\n",
        "wtText.append(FreqDist())\n",
        "maText.append(FreqDist())\n",
        "\n",
        "for d in corpus:\n",
        "    wtText.append(wtText[-1] + Text(word_tokenize(d)).vocab())\n",
        "\n",
        "    rst = list()\n",
        "    for s in sent_tokenize(re.sub(r'\\s+', ' ', d)):\n",
        "        rst.extend(ma.morphs(s))\n",
        "    maText.append(maText[-1] + Text(rst).vocab())\n",
        "wtText[-1].B(), wtText[-1].N(), \\\n",
        "maText[-1].B(), maText[-1].N()\n",
        "(14867, 33114, 7014, 62327)\n",
        "list(zip(wtText[-1].most_common(10), maText[-1].most_common(10)))\n",
        "[(('.', 1572), ('í•˜', 2556)),\n",
        " ((',', 732), ('.', 2136)),\n",
        " (('(', 390), ('ì´', 1867)),\n",
        " ((')', 390), ('ëŠ”', 1408)),\n",
        " ((\"''\", 231), ('ë‹¤', 1384)),\n",
        " (('``', 203), ('ã„´', 1365)),\n",
        " (('ìˆë‹¤', 192), ('ì„', 1303)),\n",
        " ((\"'\", 176), ('ì—', 1058)),\n",
        " (('â€œ', 167), ('ì˜', 880)),\n",
        " (('â€', 165), ('ë¥¼', 806))]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 100\n",
        "X1 = list(map(lambda r:1/(r+1), range(N)))\n",
        "X2 = list(map(lambda r:\n",
        "              wtText[-1].get(r)/wtText[-1].get(wtText[-1].max()),\n",
        "         sorted(wtText[-1], key=wtText[-1].get, reverse=True)[:N]))\n",
        "X3 = list(map(lambda r:\n",
        "              maText[-1].get(r)/maText[-1].get(maText[-1].max()),\n",
        "         sorted(maText[-1], key=maText[-1].get, reverse=True)[:N]))\n",
        "\n",
        "plt.plot(range(N), X1)\n",
        "plt.plot(range(N), X2)\n",
        "plt.plot(range(N), X3)\n",
        "[<matplotlib.lines.Line2D at 0x2a3377ca0>]\n",
        "\n",
        "sum(list(map(lambda r:\n",
        "              maText[-1].freq(r),\n",
        "         sorted(maText[-1], key=maText[-1].get, reverse=True)[:60])))\n",
        "0.4746418085259999\n",
        "maText[-1].most_common(100)[60:]\n",
        "[('ë§', 95),\n",
        " ('ëŒ€', 95),\n",
        " ('ã…‚ë‹ˆë‹¤', 94),\n",
        " ('í•œ', 93),\n",
        " ('ì‹œ', 93),\n",
        " ('3', 89),\n",
        " ('ìœ„í•˜', 89),\n",
        " ('ê·¸', 88),\n",
        " ('ì£¼', 84),\n",
        " ('ì§€ë§Œ', 84),\n",
        " ('ì—†', 82),\n",
        " ('ë³´', 81),\n",
        " ('í•œêµ­', 81),\n",
        " ('6', 79),\n",
        " ('ë‹¤ëŠ”', 77),\n",
        " ('ê² ', 76),\n",
        " ('A', 75),\n",
        " ('ì–µ', 72),\n",
        " ('ì¤‘', 72),\n",
        " ('ì¼ë³¸', 72),\n",
        " ('ë¶€í„°', 70),\n",
        " ('ë¼ê³ ', 68),\n",
        " ('ê¸°ìˆ ', 68),\n",
        " ('ì§€ë‚˜', 67),\n",
        " ('ì„œìš¸', 67),\n",
        " ('ëŒ€í†µë ¹', 66),\n",
        " ('?', 66),\n",
        " ('ë°íˆ', 66),\n",
        " ('[', 66),\n",
        " ('=', 66),\n",
        " (']', 66),\n",
        " ('â–²', 66),\n",
        " ('í†µí•˜', 65),\n",
        " ('ì•„ì„œ', 65),\n",
        " ('í¬', 65),\n",
        " ('ì„±', 65),\n",
        " ('ì‹œì¥', 65),\n",
        " ('ì›', 64),\n",
        " ('ì°¨', 63),\n",
        " ('ë˜', 62)]\n",
        "k = 22 #10-100\n",
        "b = .52\n",
        "plt.plot(range(len(maText))[1:],\n",
        "         [t.B() for t in maText][1:])\n",
        "plt.plot(range(len(maText))[1:],\n",
        "         list(map(lambda n:k*n.N()**b, maText))[1:])\n",
        "[<matplotlib.lines.Line2D at 0x2a7298610>]\n",
        "\n",
        "k*maText[-1].N()**b, maText[-1].B()\n",
        "(6849.4281921751635, 7014)\n",
        "sum(list(map(lambda r:\n",
        "              maText[-1].freq(r),\n",
        "         sorted(maText[-1], key=maText[-1].get)[:4600])))\n",
        "0.10180178734737322\n",
        "gram1 = list()\n",
        "gram2 = list()\n",
        "gram3 = list()\n",
        "\n",
        "for d in corpus:\n",
        "    rst = list()\n",
        "    for s in sent_tokenize(re.sub(r'\\s+', ' ', d)):\n",
        "        rst.extend(ma.morphs(s))\n",
        "    gram1 += ngram(rst, 1)\n",
        "    gram2 += ngram(rst, 2)\n",
        "    gram3 += ngram(rst, 3)\n",
        "len(gram1), len(gram2), len(gram3)\n",
        "(62327, 62240, 62153)\n",
        "from collections import Counter\n",
        "gram1_cnt = Counter(gram1)\n",
        "gram2_cnt = Counter(gram2)\n",
        "gram3_cnt = Counter(gram3)\n",
        "P(ë‹¨ì–´?|ì‹œì‘ë‹¨ì–´) => P(ì‹œì‘ë‹¨ì–´,ë‹¨ì–´?)/P(ì‹œì‘ë‹¨ì–´)\n",
        "               => freq(bigram(ì‹œì‘ë‹¨ì–´,ë‹¨ì–´?))/freq(unigram(ì‹œì‘ë‹¨ì–´))\n",
        "seed = 'ëŒ€í†µë ¹'\n",
        "gram1_cnt.get((seed,))\n",
        "\n",
        "list(map(lambda k:{k:gram2_cnt.get(k),\n",
        "                   'prob':gram2_cnt.get(k)/gram1_cnt.get((seed,))},\n",
        "         list(filter(lambda k:k[0] == seed, gram2_cnt))))\n",
        "[{('ëŒ€í†µë ¹', 'ì„'): 5, 'prob': 0.07575757575757576},\n",
        " {('ëŒ€í†µë ¹', 'ì—'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ê³¼'): 3, 'prob': 0.045454545454545456},\n",
        " {('ëŒ€í†µë ¹', 'í›„ë³´'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì´'): 10, 'prob': 0.15151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì‹¤'): 6, 'prob': 0.09090909090909091},\n",
        " {('ëŒ€í†µë ¹', 'ë¹„ì„œì‹¤ì¥'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì—ê²Œ'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì€'): 25, 'prob': 0.3787878787878788},\n",
        " {('ëŒ€í†µë ¹', 'ì˜'): 6, 'prob': 0.09090909090909091},\n",
        " {('ëŒ€í†µë ¹', 'ë¶€ë¶€'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì‚¬ì €'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ì‹¤ì€'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ë‹˜'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', ','): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'í‘œì°½'): 1, 'prob': 0.015151515151515152},\n",
        " {('ëŒ€í†µë ¹', 'ë°œì–¸'): 1, 'prob': 0.015151515151515152}]\n",
        "seed = 'â€œ'\n",
        "gram1_cnt.get((seed,))\n",
        "\n",
        "list(map(lambda k:{k:gram2_cnt.get(k),\n",
        "                   'prob':gram2_cnt.get(k)/gram1_cnt.get((seed,))},\n",
        "         list(filter(lambda k:k[0] == seed, gram2_cnt))))\n",
        "[{('â€œ', 'ì „'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì–´ì°¨í”¼'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì œ'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ì‚¬í›„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì €'): 4, 'prob': 0.024242424242424242},\n",
        " {('â€œ', 'ì§€ê¸ˆ'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ë‚ '): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê³§'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì´ë²ˆ'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'í˜„ì¬'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'í›„íšŒí•˜ì§€ ì•Šì•„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì–´ë–¤'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'íƒœì–´ë‚˜'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì œê°€'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì˜í™”'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', '1980ë…„ëŒ€'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í™ì½©'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê·¸ë˜ì„œ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì €ë„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìµœì‹ '): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', '('): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ë³¸ì¸'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê²Œì´ì¸ '): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë§¤ì¹´ì‹œ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì„¸ê³„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìƒˆë¡­'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í•˜ì™€ì´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì´ì—­ë§Œë¦¬'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¯¸êµ­'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë“œë¼ë§ˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í•œêµ­'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ìì´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì„¸ì¢…ëŒ€ì™•'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë°˜ë ¤'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì•ìœ¼ë¡œ'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì˜ìƒ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'êµ¬ì¡°'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê³„ë‹¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê¸°ì¡´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í–¥í›„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê±°ë˜'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì—°ê¸°ì'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì‰¬'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë‘'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', '26ë…„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¹™ë¹™'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì£¼ì˜'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì‚¬ë‘í•´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì¤‘êµ­'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë³´ì´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì½”ë¡œë‚˜'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì—¬ì„±'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì–´ì©Œë©´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í•©ì‘'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì§€ë‚œí•´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'â€˜'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë””ì§€í„¸'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìœ¤'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë„ìš”íƒ€'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì¥ê¸°'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'OLED'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê°€ì¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', '2024ë…„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'UDC'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'B'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì„¤ê³„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê¹€ê±´í¬'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ê·¸'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ì˜¤ëŠ˜'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì—°ê°„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìš°ë¦¬'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¦¬ëª¨ì»¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê¶ê·¹'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìŠ¤íƒ€íŠ¸'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í•œêµ­ì˜ ì „í†µ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í˜‘ë™'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì§€ë‚˜'): 3, 'prob': 0.01818181818181818},\n",
        " {('â€œ', 'ë¹ ë¥´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'êµ­ì±„'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì—¬ì „íˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë³´ë‹¤'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê³„ì†'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì—”'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê²½ê°ì‹¬'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'êµ­ì œ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìƒë‹¹íˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê°€ê³„ë¶€ì±„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì˜¬í•´'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ê³ ê¸ˆë¦¬'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë‚´ë…„'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ìƒì„±'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'AI'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì†Œìœ '): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì˜ˆì „'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ì•„ë§ˆì¡´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¶„ì´ˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì´ˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê³ ë ¹ì'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ëŒ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë²”ì£„ì'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê°€'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë„ì €íˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì‚¬í‡´'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ìœ„ì›ì¥'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ë‚˜ê°€'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í›„ë³´ì'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê¹€'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ë¼ê³ '): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì½”ë ˆì¼ìœ í†µ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì„ ë„'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'í•™ìƒ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ëŒ€í•™'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì²˜ìŒ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'R'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¬´ìŠ¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì¬ì •'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'êµ­íšŒ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê·¸ë™ì•ˆ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'êµìœ¡'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê¸´ì¶•'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë°±ì„œ'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¸”ë™ë¦¬ìŠ¤íŠ¸'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ëª¨ë‘'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ë¯¼ì£¼ë‹¹'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ê·¸ëŸ°'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì—¬ê¸°'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'OO'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ì›ìƒ‰'): 1, 'prob': 0.006060606060606061},\n",
        " {('â€œ', 'ëŒ€ë²•ì›ì¥'): 2, 'prob': 0.012121212121212121},\n",
        " {('â€œ', 'ë¶€ë””'): 1, 'prob': 0.006060606060606061}]\n",
        "# keyë¡œ ì‹œì‘í•˜ëŠ” freq ì°¾ëŠ” í•¨ìˆ˜\n",
        "def find(key, gram):\n",
        "    n = len(key)\n",
        "    return [(k,gram.get(k)) for k in list(gram.keys())\n",
        "            if k[:n] == key]\n",
        "# P(ABC) => P(C|AB)\n",
        "#        => P(C|A?)\\\n",
        "seed = ('ëŒ€í†µë ¹',)\n",
        "for i in range(20):\n",
        "    n = len(seed)\n",
        "    if n==1:\n",
        "        p1=gram1_cnt\n",
        "        p2=gram2_cnt\n",
        "    elif n==2:\n",
        "        p1=gram2_cnt\n",
        "        p2=gram3_cnt\n",
        "\n",
        "    given = find(seed, p1)[0][1]\n",
        "    candidates = list(map(lambda r:(r[0], r[1]/given),\n",
        "                          find(seed, p2)))\n",
        "    best = list(sorted(candidates, key=lambda r:r[1],\n",
        "                       reverse=True))[0]\n",
        "    print(best)\n",
        "    seed = best[0][-2:]\n",
        "(('ëŒ€í†µë ¹', 'ì€'), 0.3787878787878788)\n",
        "(('ëŒ€í†µë ¹', 'ì€', '\"'), 0.2)\n",
        "(('ì€', '\"', 'ì™¸êµ­'), 0.043478260869565216)\n",
        "(('\"', 'ì™¸êµ­', 'ê³„'), 1.0)\n",
        "(('ì™¸êµ­', 'ê³„', 'ê³µë£¡'), 0.4)\n",
        "(('ê³„', 'ê³µë£¡', 'í”Œë«í¼'), 1.0)\n",
        "(('ê³µë£¡', 'í”Œë«í¼', 'ì‹¤íƒœ'), 0.5)\n",
        "(('í”Œë«í¼', 'ì‹¤íƒœ', 'íŒŒì•…'), 1.0)\n",
        "(('ì‹¤íƒœ', 'íŒŒì•…', 'â€¦'), 0.5)\n",
        "(('íŒŒì•…', 'â€¦', 'ì¡°ì„¸'), 1.0)\n",
        "(('â€¦', 'ì¡°ì„¸', 'ì •'), 1.0)\n",
        "(('ì¡°ì„¸', 'ì •', 'ì˜'), 1.0)\n",
        "(('ì •', 'ì˜', 'ì‹¤í˜„'), 1.0)\n",
        "(('ì˜', 'ì‹¤í˜„', 'ì‹œê¸‰'), 0.3333333333333333)\n",
        "(('ì‹¤í˜„', 'ì‹œê¸‰', '\"'), 1.0)\n",
        "(('ì‹œê¸‰', '\"', '2004'), 1.0)\n",
        "(('\"', '2004', 'ë…„'), 1.0)\n",
        "(('2004', 'ë…„', 'ì„¤ë¦½'), 0.5)\n",
        "(('ë…„', 'ì„¤ë¦½', 'ë˜'), 1.0)\n",
        "(('ì„¤ë¦½', 'ë˜', 'ã„´'), 1.0)\n",
        "# ê³µë°±ì´ í¬í•¨ëœ ngram\n",
        "ngram1 = Counter(ngram(' '.join(corpus), 1))\n",
        "ngram2 = Counter(ngram(' '.join(corpus), 2))\n",
        "ngram3 = Counter(ngram(' '.join(corpus), 3))\n",
        "ngram4 = Counter(ngram(' '.join(corpus), 4))\n",
        "# ìë™ ë„ì–´ì“°ê¸°\n",
        "'ì„ëª… ë™ì˜ì•ˆì´ ìƒë‹¹ ê¸°ê°„ í‘œë¥˜í•œ ë°ë‹¤ ê³¼ë°˜ ì˜ì„ì„ ê°€ì§„ ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ì´ ë¶€ê²° ì¹´ë“œë¥¼ ë§Œì§€ì‘ê±°ë¦¬ë©´ì„œ ì •ì¹˜ê¶Œì´ ë‹¹ë¦¬ë‹¹ëµì— ë”°ë¼ ì‚¬ë²•ë¶€ ìˆ˜ì¥ì„ ë³¼ëª¨ë¡œ ì¡ì•„ì„  ì•ˆ ëœë‹¤ëŠ” ì§€ì ì´ë‹¤.'.replace(' ', '')\n",
        "'ì„ëª…ë™ì˜ì•ˆì´ìƒë‹¹ê¸°ê°„í‘œë¥˜í•œë°ë‹¤ê³¼ë°˜ì˜ì„ì„ê°€ì§„ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ì´ë¶€ê²°ì¹´ë“œë¥¼ë§Œì§€ì‘ê±°ë¦¬ë©´ì„œì •ì¹˜ê¶Œì´ë‹¹ë¦¬ë‹¹ëµì—ë”°ë¼ì‚¬ë²•ë¶€ìˆ˜ì¥ì„ë³¼ëª¨ë¡œì¡ì•„ì„ ì•ˆëœë‹¤ëŠ”ì§€ì ì´ë‹¤.'\n",
        "find(('ì„',), ngram1)\n",
        "[(('ì„',), 74)]\n",
        "list(sorted(find(tuple('ë™ì˜'), ngram3),\n",
        "            key=lambda r:r[1], reverse=True))[0]\n",
        "(('ë™', 'ì˜', ' '), 4)\n",
        "Ngram => LanguageModel\n",
        "ë¹ˆë„ => MLE => Conditional Prob. (ê³¼ì •)ìì²´ê°€ NLU\n",
        "Ngrams => ë‹¤ìŒ ë‹¨ì–´ëŠ”? ìµœëŒ€í™•ë¥ ì¶”ì • NLG\n",
        "=> Tokenizing\n",
        "image.png\n",
        "\n",
        "P(ìˆ|ì•Œì˜ë”±ê¹”) => 1\n",
        "P(?|ì•Œì˜ë”±ê¹”) => 1/0.~ => > 1,\n",
        "P(ìˆ|ì•Œì˜ë”±ê¹”) => P(ì•Œì˜ë”±ê¹”ìˆ)/P(ì•Œì˜ë”±ê¹”)\n",
        "P(ì˜|ì•Œ)*P(ë”±|ì•Œì˜)*P(ê¹”|ì•Œì˜ë”±)*P(ìˆ|ì•Œì˜ë”±ê¹”)\n",
        "P(ì•Œì˜)   P(ì•Œì˜ë”±)   P(ì•Œì˜ë”±ê¹”)       P(ì „ì²´ì¡°í•©)\n",
        "------ X ------- X -------    => ----------\n",
        "P(ì•Œ)     P(ì•Œì˜)   P (ì•Œì˜ë”±)        P(ì²«ê¸€ì)\n",
        "=> Cohesion Score => ë¶„ëª¨ = 1\n",
        "find(tuple('ë ˆ'), ngram1)\n",
        "[(('ë ˆ',), 130)]\n",
        "find(tuple('ë ˆì´'), ngram2)\n",
        "[(('ë ˆ', 'ì´'), 81)]\n",
        "find(tuple('ë ˆì´'), ngram3)\n",
        "[(('ë ˆ', 'ì´', ' '), 46),\n",
        " (('ë ˆ', 'ì´', \"'\"), 1),\n",
        " (('ë ˆ', 'ì´', 'ì˜'), 9),\n",
        " (('ë ˆ', 'ì´', 'ë¼'), 1),\n",
        " (('ë ˆ', 'ì´', 'ëŠ”'), 6),\n",
        " (('ë ˆ', 'ì´', 'í¬'), 1),\n",
        " (('ë ˆ', 'ì´', 'ì…˜'), 3),\n",
        " (('ë ˆ', 'ì´', 'ë¥¼'), 3),\n",
        " (('ë ˆ', 'ì´', 'ê°€'), 1),\n",
        " (('ë ˆ', 'ì´', 'ì €'), 1),\n",
        " (('ë ˆ', 'ì´', 'ì—'), 2),\n",
        " (('ë ˆ', 'ì´', ','), 1),\n",
        " (('ë ˆ', 'ì´', 'ìŠ¤'), 1),\n",
        " (('ë ˆ', 'ì´', 'ì‚°'), 1),\n",
        " (('ë ˆ', 'ì´', '.'), 1),\n",
        " (('ë ˆ', 'ì´', '['), 1),\n",
        " (('ë ˆ', 'ì´', 'í•™'), 1),\n",
        " (('ë ˆ', 'ì´', 'ì‹œ'), 1)]\n",
        "81/130, (46/81)*(1/81)*(9/81)\n",
        "(0.6230769230769231, 0.0007790140391877931)\n",
        "[ì–´ê·¼/ì–´ê°„ í˜•íƒœì†Œ]+í˜•íƒœì†Œ(í˜•ì‹/ì˜ì¡´)\n",
        "*     |\n",
        "  ****|\n",
        "      |***\n",
        "ëŒ€ í†µ ë ¹  ì´\n",
        "        ê»˜ì„œ.."
      ]
    }
  ]
}