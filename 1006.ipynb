{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC2mgvCViBsHZzTMHUzmfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEi8BHDAXLBZ"
      },
      "outputs": [],
      "source": [
        "Tokenizing\n",
        "[Feature Extraction]\n",
        "- sent_tokenize; Î¨∏Ïû•(Íµ¨ÎëêÏ†ê) => Î¨∏Îã® => Î¨∏ÏÑú\n",
        "- word_tokenize; regex; tweet; term/token/word/subword ...\n",
        "                 => Ïö©Ïñ¥ => Ïñ¥ÌúòÏßëÌï©(Voca)\n",
        "                 => Text.vocab() = FreqDist = Count\n",
        "[Featrue Selection]\n",
        "=> Zipf; ÏûêÏó∞Ïñ¥, ÏàúÏúÑÏùò Ïó≠Í≥º ÎπàÎèÑÎ°ú ÎÇòÏó¥ÌñàÏùÑÎïå Í∞ôÎçîÎùº, Ï†Ñ ÏàúÏúÑÎ≥¥Îã§ 2Î∞∞ Í∞ÄÎüâ ÎßéÏù¥ ÎÇòÏò¥\n",
        "       ; Í≥†ÎπàÎèÑ(50%, ÏÉÅÏúÑ ÏÜåÏàòÏùò Îã®Ïñ¥) - Ï§ëÎπàÎèÑ(*) - Ï†ÄÎπàÎèÑ(25%, Ìù¨ÏÜåÌïú)\n",
        "         => TF-IDF, PRP->Okapi->BM25 => TF-IDF\n",
        "=> Heaps; corpusÍ∞Ä Ïª§ÏßàÏàòÎ°ù unique Îã®Ïñ¥Îì§Ïù¥ (ÌäπÏ†ï Î≤ïÏπô) Ï¶ùÍ∞Ä\n",
        "        ; k,b=>.4~.6 => feature ÏàòÎ•º ÏòàÏ∏° / Zipf ÏñëÏßàÏùò feature\n",
        "\n",
        "[Korean]\n",
        "- Morpheme/POS\n",
        "  -> ÌòïÌÉúÏÜå/ÌòïÌÉúÏÜå/ÌòïÌÉúÏÜå\n",
        "  -> NN    VV   EX\n",
        "- Grammar -> Rules => RNN\n",
        "  -> NP      VP  => S\n",
        "=> N-gram(NÍ∞úÏùò gram=token) => LM\n",
        "   1(uni), 2(bi), 3(tri), 4(...), ... (X)\n",
        "ùë∑(ùíôùíä|ùíôùíä‚àí ùíè‚àíùüè ,...,ùíôùíä‚àíùüè) => N-gram P(A,B) = ? A=Î∞∞, B=Í≥†ÌîÑÎã§, Í≤ΩÏö∞Ïùò ÏàòÎ•º Î™®Îëê Ï°∞Ìï© => X P(A,B) => P(B|A)P(A) P(B|A) => P(A,B)/P(A) ------ ---- bigram uigram Î∞∞+?; MLE P(freq(A+B)/freq(A)) N N (Îã®Ïñ¥,Îã®Ïñ¥), (Îã®Ïñ¥,Îã®Ïñ¥) => LM P(C|A,B) => P(A,B,C)/P(A,B) => (Îã®Ïñ¥,Îã®Ïñ¥,Îã®Ïñ¥) P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... => 0 Markov Assumption(1st, 2nd, ..) P(A,B,C) => P(C|A,B)P(B|A)P(A) ~= P(B,C) P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... ~= P(Z|Y)P(Y|X)P(X|..)... 1st Markov Assumption => Bigram 2nd Markov Assumption => Trigram P(A,B,C,D,E,F,G,H,...,Z) => P(Z|A,...,Y)P(Y|A,...,X)... ~= P(Z|XY)P(Y|WX)P(X|VW)... Z|XA, XB, XC, .., XY ABC ABD=0 ABE ACA ACG ACE=0 ... => ÏïÑ ___ => Sequence(L;given->R) => RNN;LSTM;GRU / Attention -----> <----- A B C [mask] E F G; LM? => Auto-encoder - - - - P(A) P(B|A) ... A B C D E F ...\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(tuple(s[i:i+n]))\n",
        "    return rst\n",
        "from konlpy.corpus import kolaw\n",
        "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
        "from konlpy.tag import Komoran\n",
        "ma = Komoran()\n",
        "Python        JPype        Java(JVM)\n",
        "Komoran() --------------> Komoran()\n",
        "id        <--------------\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "tokens = word_tokenize(corpus)\n",
        "bigram = ngram(tokens)\n",
        "biText = Text(bigram)\n",
        "unigram = ngram(tokens, 1)\n",
        "len(list(filter(lambda t:t[0] == 'Î≤ïÎ•†Ïù¥', unigram)))\n",
        "57\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "a = FreqDist(list(filter(lambda t:t[0] == 'Î≤ïÎ•†Ïù¥', bigram)))\n",
        "for k, v in a.items():\n",
        "    print(f'{k}-{v} => P({k}|Î≤ïÎ•†Ïù¥)={v/sum(a.values())}')\n",
        "('Î≤ïÎ•†Ïù¥', 'Ï†ïÌïòÎäî')-48 => P(('Î≤ïÎ•†Ïù¥', 'Ï†ïÌïòÎäî')|Î≤ïÎ•†Ïù¥)=0.8421052631578947\n",
        "('Î≤ïÎ•†Ïù¥', 'Ï†ïÌïú')-7 => P(('Î≤ïÎ•†Ïù¥', 'Ï†ïÌïú')|Î≤ïÎ•†Ïù¥)=0.12280701754385964\n",
        "('Î≤ïÎ•†Ïù¥', 'ÌôïÏ†ïÎêú')-1 => P(('Î≤ïÎ•†Ïù¥', 'ÌôïÏ†ïÎêú')|Î≤ïÎ•†Ïù¥)=0.017543859649122806\n",
        "('Î≤ïÎ•†Ïù¥', 'ÌóåÎ≤ïÏóê')-1 => P(('Î≤ïÎ•†Ïù¥', 'ÌóåÎ≤ïÏóê')|Î≤ïÎ•†Ïù¥)=0.017543859649122806\n",
        "# ÎÑ§Ïù¥Î≤Ñ Îâ¥Ïä§ ÏàòÏßëÍ∏∞ - Crawler\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "urls = list()\n",
        "seens = list()\n",
        "\n",
        "path = 'naver/'\n",
        "urls.append('https://news.naver.com/')\n",
        "\n",
        "headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}\n",
        "\n",
        "while urls:\n",
        "    url = urls.pop(0)\n",
        "    seens.append(url)\n",
        "\n",
        "    dom = BeautifulSoup(get(url,headers=headers).text, 'html.parser')\n",
        "\n",
        "    # Î©îÎâ¥\n",
        "    alist = dom.select('ul[role=menu] > li > a[href]')\n",
        "    for a in alist:\n",
        "        if re.search(r'sid1=\\d{3}$', a.attrs['href']):\n",
        "            if a.attrs['href'] not in urls and\\\n",
        "               a.attrs['href'] not in seens:\n",
        "                urls.append(a.attrs['href'])\n",
        "\n",
        "    # Í∏∞ÏÇ¨ÎßÅÌÅ¨ => Ï†ïÍµêÌïòÍ≤å\n",
        "    alist = dom.select('''\n",
        "                        .sh_text > a[href],\n",
        "                        .cluster_text > a[href],\n",
        "                        dt > a[href]\n",
        "                    ''')\n",
        "    for a in alist:\n",
        "        if a.attrs['href'] not in urls and\\\n",
        "           a.attrs['href'] not in seens:\n",
        "            urls.append(a.attrs['href'])\n",
        "\n",
        "    # Î≥∏Î¨∏\n",
        "    news = dom.select_one('#newsct_article')\n",
        "    if news:\n",
        "        file = re.search(r'(\\d{10})', url).group(1)\n",
        "        with open(f'{path}{file}.txt', 'w', encoding='utf-8') as fp:\n",
        "            fp.write(news.text)\n",
        "from os import listdir, mkdir\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "corpus = list()\n",
        "for f in fileids('naver'):\n",
        "    with open(f, 'r', ecoding='utf8') as fp:\n",
        "        corpus.append(fp.read())\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "wtText = list()\n",
        "maText = list()\n",
        "wtText.append(FreqDist())\n",
        "maText.append(FreqDist())\n",
        "\n",
        "for d in corpus:\n",
        "    wtText.append(wtText[-1] + Text(word_tokenize(d)).vocab())\n",
        "\n",
        "    rst = list()\n",
        "    for s in sent_tokenize(re.sub(r'\\s+', ' ', d)):\n",
        "        rst.extend(ma.morphs(s))\n",
        "    maText.append(maText[-1] + Text(rst).vocab())\n",
        "wtText[-1].B(), wtText[-1].N(), \\\n",
        "maText[-1].B(), maText[-1].N()\n",
        "(14867, 33114, 7014, 62327)\n",
        "list(zip(wtText[-1].most_common(10), maText[-1].most_common(10)))\n",
        "[(('.', 1572), ('Ìïò', 2556)),\n",
        " ((',', 732), ('.', 2136)),\n",
        " (('(', 390), ('Ïù¥', 1867)),\n",
        " ((')', 390), ('Îäî', 1408)),\n",
        " ((\"''\", 231), ('Îã§', 1384)),\n",
        " (('``', 203), ('„Ñ¥', 1365)),\n",
        " (('ÏûàÎã§', 192), ('ÏùÑ', 1303)),\n",
        " ((\"'\", 176), ('Ïóê', 1058)),\n",
        " (('‚Äú', 167), ('Ïùò', 880)),\n",
        " (('‚Äù', 165), ('Î•º', 806))]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 100\n",
        "X1 = list(map(lambda r:1/(r+1), range(N)))\n",
        "X2 = list(map(lambda r:\n",
        "              wtText[-1].get(r)/wtText[-1].get(wtText[-1].max()),\n",
        "         sorted(wtText[-1], key=wtText[-1].get, reverse=True)[:N]))\n",
        "X3 = list(map(lambda r:\n",
        "              maText[-1].get(r)/maText[-1].get(maText[-1].max()),\n",
        "         sorted(maText[-1], key=maText[-1].get, reverse=True)[:N]))\n",
        "\n",
        "plt.plot(range(N), X1)\n",
        "plt.plot(range(N), X2)\n",
        "plt.plot(range(N), X3)\n",
        "[<matplotlib.lines.Line2D at 0x2a3377ca0>]\n",
        "\n",
        "sum(list(map(lambda r:\n",
        "              maText[-1].freq(r),\n",
        "         sorted(maText[-1], key=maText[-1].get, reverse=True)[:60])))\n",
        "0.4746418085259999\n",
        "maText[-1].most_common(100)[60:]\n",
        "[('Îßê', 95),\n",
        " ('ÎåÄ', 95),\n",
        " ('„ÖÇÎãàÎã§', 94),\n",
        " ('Ìïú', 93),\n",
        " ('Ïãú', 93),\n",
        " ('3', 89),\n",
        " ('ÏúÑÌïò', 89),\n",
        " ('Í∑∏', 88),\n",
        " ('Ï£º', 84),\n",
        " ('ÏßÄÎßå', 84),\n",
        " ('ÏóÜ', 82),\n",
        " ('Î≥¥', 81),\n",
        " ('ÌïúÍµ≠', 81),\n",
        " ('6', 79),\n",
        " ('Îã§Îäî', 77),\n",
        " ('Í≤†', 76),\n",
        " ('A', 75),\n",
        " ('Ïñµ', 72),\n",
        " ('Ï§ë', 72),\n",
        " ('ÏùºÎ≥∏', 72),\n",
        " ('Î∂ÄÌÑ∞', 70),\n",
        " ('ÎùºÍ≥†', 68),\n",
        " ('Í∏∞Ïà†', 68),\n",
        " ('ÏßÄÎÇò', 67),\n",
        " ('ÏÑúÏö∏', 67),\n",
        " ('ÎåÄÌÜµÎ†π', 66),\n",
        " ('?', 66),\n",
        " ('Î∞ùÌûà', 66),\n",
        " ('[', 66),\n",
        " ('=', 66),\n",
        " (']', 66),\n",
        " ('‚ñ≤', 66),\n",
        " ('ÌÜµÌïò', 65),\n",
        " ('ÏïÑÏÑú', 65),\n",
        " ('ÌÅ¨', 65),\n",
        " ('ÏÑ±', 65),\n",
        " ('ÏãúÏû•', 65),\n",
        " ('Ïõê', 64),\n",
        " ('Ï∞®', 63),\n",
        " ('Îçò', 62)]\n",
        "k = 22 #10-100\n",
        "b = .52\n",
        "plt.plot(range(len(maText))[1:],\n",
        "         [t.B() for t in maText][1:])\n",
        "plt.plot(range(len(maText))[1:],\n",
        "         list(map(lambda n:k*n.N()**b, maText))[1:])\n",
        "[<matplotlib.lines.Line2D at 0x2a7298610>]\n",
        "\n",
        "k*maText[-1].N()**b, maText[-1].B()\n",
        "(6849.4281921751635, 7014)\n",
        "sum(list(map(lambda r:\n",
        "              maText[-1].freq(r),\n",
        "         sorted(maText[-1], key=maText[-1].get)[:4600])))\n",
        "0.10180178734737322\n",
        "gram1 = list()\n",
        "gram2 = list()\n",
        "gram3 = list()\n",
        "\n",
        "for d in corpus:\n",
        "    rst = list()\n",
        "    for s in sent_tokenize(re.sub(r'\\s+', ' ', d)):\n",
        "        rst.extend(ma.morphs(s))\n",
        "    gram1 += ngram(rst, 1)\n",
        "    gram2 += ngram(rst, 2)\n",
        "    gram3 += ngram(rst, 3)\n",
        "len(gram1), len(gram2), len(gram3)\n",
        "(62327, 62240, 62153)\n",
        "from collections import Counter\n",
        "gram1_cnt = Counter(gram1)\n",
        "gram2_cnt = Counter(gram2)\n",
        "gram3_cnt = Counter(gram3)\n",
        "P(Îã®Ïñ¥?|ÏãúÏûëÎã®Ïñ¥) => P(ÏãúÏûëÎã®Ïñ¥,Îã®Ïñ¥?)/P(ÏãúÏûëÎã®Ïñ¥)\n",
        "               => freq(bigram(ÏãúÏûëÎã®Ïñ¥,Îã®Ïñ¥?))/freq(unigram(ÏãúÏûëÎã®Ïñ¥))\n",
        "seed = 'ÎåÄÌÜµÎ†π'\n",
        "gram1_cnt.get((seed,))\n",
        "\n",
        "list(map(lambda k:{k:gram2_cnt.get(k),\n",
        "                   'prob':gram2_cnt.get(k)/gram1_cnt.get((seed,))},\n",
        "         list(filter(lambda k:k[0] == seed, gram2_cnt))))\n",
        "[{('ÎåÄÌÜµÎ†π', 'ÏùÑ'): 5, 'prob': 0.07575757575757576},\n",
        " {('ÎåÄÌÜµÎ†π', 'Ïóê'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Í≥º'): 3, 'prob': 0.045454545454545456},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÌõÑÎ≥¥'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Ïù¥'): 10, 'prob': 0.15151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Ïã§'): 6, 'prob': 0.09090909090909091},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÎπÑÏÑúÏã§Ïû•'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÏóêÍ≤å'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÏùÄ'): 25, 'prob': 0.3787878787878788},\n",
        " {('ÎåÄÌÜµÎ†π', 'Ïùò'): 6, 'prob': 0.09090909090909091},\n",
        " {('ÎåÄÌÜµÎ†π', 'Î∂ÄÎ∂Ä'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÏÇ¨Ï†Ä'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Ïã§ÏùÄ'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Îãò'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', ','): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'ÌëúÏ∞Ω'): 1, 'prob': 0.015151515151515152},\n",
        " {('ÎåÄÌÜµÎ†π', 'Î∞úÏñ∏'): 1, 'prob': 0.015151515151515152}]\n",
        "seed = '‚Äú'\n",
        "gram1_cnt.get((seed,))\n",
        "\n",
        "list(map(lambda k:{k:gram2_cnt.get(k),\n",
        "                   'prob':gram2_cnt.get(k)/gram1_cnt.get((seed,))},\n",
        "         list(filter(lambda k:k[0] == seed, gram2_cnt))))\n",
        "[{('‚Äú', 'Ï†Ñ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïñ¥Ï∞®Ìîº'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï†ú'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'ÏÇ¨ÌõÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï†Ä'): 4, 'prob': 0.024242424242424242},\n",
        " {('‚Äú', 'ÏßÄÍ∏à'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÎÇ†'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≥ß'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïù¥Î≤à'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÌòÑÏû¨'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'ÌõÑÌöåÌïòÏßÄ ÏïäÏïÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïñ¥Îñ§'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌÉúÏñ¥ÎÇò'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï†úÍ∞Ä'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÏòÅÌôî'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', '1980ÎÖÑÎåÄ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌôçÏΩ©'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∑∏ÎûòÏÑú'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï†ÄÎèÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïù¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏµúÏã†'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', '('): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'Î≥∏Ïù∏'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≤åÏù¥Ï∏†'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Îß§Ïπ¥Ïãú'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÑ∏Í≥Ñ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÉàÎ°≠'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌïòÏôÄÏù¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïù¥Ïó≠ÎßåÎ¶¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎØ∏Íµ≠'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎìúÎùºÎßà'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌïúÍµ≠'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'ÏûêÏù¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÑ∏Ï¢ÖÎåÄÏôï'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î∞òÎ†§'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏïûÏúºÎ°ú'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÏòÅÏÉÅ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Íµ¨Ï°∞'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≥ÑÎã®'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∏∞Ï°¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ìñ•ÌõÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í±∞Îûò'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïó∞Í∏∞Ïûê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïâ¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Îëê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', '26ÎÖÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎπôÎπô'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Ï£ºÏòÅ'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÏÇ¨ÎûëÌï¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï§ëÍµ≠'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î≥¥Ïù¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏΩîÎ°úÎÇò'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Ïó¨ÏÑ±'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïñ¥Ï©åÎ©¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ìï©Ïûë'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏßÄÎÇúÌï¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', '‚Äò'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎîîÏßÄÌÑ∏'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïú§'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎèÑÏöîÌÉÄ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïû•Í∏∞'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'OLED'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∞ÄÏû•'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ìè¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', '2024ÎÖÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'UDC'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'B'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÑ§Í≥Ñ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÍπÄÍ±¥Ìù¨'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'Í∑∏'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'Ïò§Îäò'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïó∞Í∞Ñ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïö∞Î¶¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î¶¨Î™®Ïª®'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∂ÅÍ∑π'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïä§ÌÉÄÌä∏'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌïúÍµ≠Ïùò Ï†ÑÌÜµ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌòëÎèô'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏßÄÎÇò'): 3, 'prob': 0.01818181818181818},\n",
        " {('‚Äú', 'Îπ†Î•¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Íµ≠Ï±Ñ'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Ïó¨Ï†ÑÌûà'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î≥¥Îã§'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≥ÑÏÜç'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïóî'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≤ΩÍ∞ÅÏã¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Íµ≠Ï†ú'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÉÅÎãπÌûà'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∞ÄÍ≥ÑÎ∂ÄÏ±Ñ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïò¨Ìï¥'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Í≥†Í∏àÎ¶¨'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎÇ¥ÎÖÑ'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÏÉùÏÑ±'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'AI'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÜåÏú†'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏòàÏ†Ñ'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÏïÑÎßàÏ°¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î∂ÑÏ¥à'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï¥à'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í≥†Î†πÏûê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Îèå'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î≤îÏ£ÑÏûê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∞Ä'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎèÑÏ†ÄÌûà'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÇ¨Ìá¥'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏúÑÏõêÏû•'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÎÇòÍ∞Ä'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌõÑÎ≥¥Ïûê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÍπÄ'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'ÎùºÍ≥†'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏΩîÎ†àÏùºÏú†ÌÜµ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏÑ†ÎèÑ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÌïôÏÉù'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎåÄÌïô'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ï≤òÏùå'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'R'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î¨¥Ïä®'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïû¨Ï†ï'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Íµ≠Ìöå'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∑∏ÎèôÏïà'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÍµêÏú°'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∏¥Ï∂ï'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î∞±ÏÑú'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Î∏îÎûôÎ¶¨Ïä§Ìä∏'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Î™®Îëê'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎØºÏ£ºÎãπ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Í∑∏Îü∞'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'Ïó¨Í∏∞'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'OO'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÏõêÏÉâ'): 1, 'prob': 0.006060606060606061},\n",
        " {('‚Äú', 'ÎåÄÎ≤ïÏõêÏû•'): 2, 'prob': 0.012121212121212121},\n",
        " {('‚Äú', 'Î∂ÄÎîî'): 1, 'prob': 0.006060606060606061}]\n",
        "# keyÎ°ú ÏãúÏûëÌïòÎäî freq Ï∞æÎäî Ìï®Ïàò\n",
        "def find(key, gram):\n",
        "    n = len(key)\n",
        "    return [(k,gram.get(k)) for k in list(gram.keys())\n",
        "            if k[:n] == key]\n",
        "# P(ABC) => P(C|AB)\n",
        "#        => P(C|A?)\\\n",
        "seed = ('ÎåÄÌÜµÎ†π',)\n",
        "for i in range(20):\n",
        "    n = len(seed)\n",
        "    if n==1:\n",
        "        p1=gram1_cnt\n",
        "        p2=gram2_cnt\n",
        "    elif n==2:\n",
        "        p1=gram2_cnt\n",
        "        p2=gram3_cnt\n",
        "\n",
        "    given = find(seed, p1)[0][1]\n",
        "    candidates = list(map(lambda r:(r[0], r[1]/given),\n",
        "                          find(seed, p2)))\n",
        "    best = list(sorted(candidates, key=lambda r:r[1],\n",
        "                       reverse=True))[0]\n",
        "    print(best)\n",
        "    seed = best[0][-2:]\n",
        "(('ÎåÄÌÜµÎ†π', 'ÏùÄ'), 0.3787878787878788)\n",
        "(('ÎåÄÌÜµÎ†π', 'ÏùÄ', '\"'), 0.2)\n",
        "(('ÏùÄ', '\"', 'Ïô∏Íµ≠'), 0.043478260869565216)\n",
        "(('\"', 'Ïô∏Íµ≠', 'Í≥Ñ'), 1.0)\n",
        "(('Ïô∏Íµ≠', 'Í≥Ñ', 'Í≥µÎ£°'), 0.4)\n",
        "(('Í≥Ñ', 'Í≥µÎ£°', 'ÌîåÎû´Ìèº'), 1.0)\n",
        "(('Í≥µÎ£°', 'ÌîåÎû´Ìèº', 'Ïã§ÌÉú'), 0.5)\n",
        "(('ÌîåÎû´Ìèº', 'Ïã§ÌÉú', 'ÌååÏïÖ'), 1.0)\n",
        "(('Ïã§ÌÉú', 'ÌååÏïÖ', '‚Ä¶'), 0.5)\n",
        "(('ÌååÏïÖ', '‚Ä¶', 'Ï°∞ÏÑ∏'), 1.0)\n",
        "(('‚Ä¶', 'Ï°∞ÏÑ∏', 'Ï†ï'), 1.0)\n",
        "(('Ï°∞ÏÑ∏', 'Ï†ï', 'Ïùò'), 1.0)\n",
        "(('Ï†ï', 'Ïùò', 'Ïã§ÌòÑ'), 1.0)\n",
        "(('Ïùò', 'Ïã§ÌòÑ', 'ÏãúÍ∏â'), 0.3333333333333333)\n",
        "(('Ïã§ÌòÑ', 'ÏãúÍ∏â', '\"'), 1.0)\n",
        "(('ÏãúÍ∏â', '\"', '2004'), 1.0)\n",
        "(('\"', '2004', 'ÎÖÑ'), 1.0)\n",
        "(('2004', 'ÎÖÑ', 'ÏÑ§Î¶Ω'), 0.5)\n",
        "(('ÎÖÑ', 'ÏÑ§Î¶Ω', 'Îêò'), 1.0)\n",
        "(('ÏÑ§Î¶Ω', 'Îêò', '„Ñ¥'), 1.0)\n",
        "# Í≥µÎ∞±Ïù¥ Ìè¨Ìï®Îêú ngram\n",
        "ngram1 = Counter(ngram(' '.join(corpus), 1))\n",
        "ngram2 = Counter(ngram(' '.join(corpus), 2))\n",
        "ngram3 = Counter(ngram(' '.join(corpus), 3))\n",
        "ngram4 = Counter(ngram(' '.join(corpus), 4))\n",
        "# ÏûêÎèô ÎùÑÏñ¥Ïì∞Í∏∞\n",
        "'ÏûÑÎ™Ö ÎèôÏùòÏïàÏù¥ ÏÉÅÎãπ Í∏∞Í∞Ñ ÌëúÎ•òÌïú Îç∞Îã§ Í≥ºÎ∞ò ÏùòÏÑùÏùÑ Í∞ÄÏßÑ ÎçîÎ∂àÏñ¥ÎØºÏ£ºÎãπÏù¥ Î∂ÄÍ≤∞ Ïπ¥ÎìúÎ•º ÎßåÏßÄÏûëÍ±∞Î¶¨Î©¥ÏÑú Ï†ïÏπòÍ∂åÏù¥ ÎãπÎ¶¨ÎãπÎûµÏóê Îî∞Îùº ÏÇ¨Î≤ïÎ∂Ä ÏàòÏû•ÏùÑ Î≥ºÎ™®Î°ú Ïû°ÏïÑÏÑ† Ïïà ÎêúÎã§Îäî ÏßÄÏ†ÅÏù¥Îã§.'.replace(' ', '')\n",
        "'ÏûÑÎ™ÖÎèôÏùòÏïàÏù¥ÏÉÅÎãπÍ∏∞Í∞ÑÌëúÎ•òÌïúÎç∞Îã§Í≥ºÎ∞òÏùòÏÑùÏùÑÍ∞ÄÏßÑÎçîÎ∂àÏñ¥ÎØºÏ£ºÎãπÏù¥Î∂ÄÍ≤∞Ïπ¥ÎìúÎ•ºÎßåÏßÄÏûëÍ±∞Î¶¨Î©¥ÏÑúÏ†ïÏπòÍ∂åÏù¥ÎãπÎ¶¨ÎãπÎûµÏóêÎî∞ÎùºÏÇ¨Î≤ïÎ∂ÄÏàòÏû•ÏùÑÎ≥ºÎ™®Î°úÏû°ÏïÑÏÑ†ÏïàÎêúÎã§ÎäîÏßÄÏ†ÅÏù¥Îã§.'\n",
        "find(('ÏûÑ',), ngram1)\n",
        "[(('ÏûÑ',), 74)]\n",
        "list(sorted(find(tuple('ÎèôÏùò'), ngram3),\n",
        "            key=lambda r:r[1], reverse=True))[0]\n",
        "(('Îèô', 'Ïùò', ' '), 4)\n",
        "Ngram => LanguageModel\n",
        "ÎπàÎèÑ => MLE => Conditional Prob. (Í≥ºÏ†ï)ÏûêÏ≤¥Í∞Ä NLU\n",
        "Ngrams => Îã§Ïùå Îã®Ïñ¥Îäî? ÏµúÎåÄÌôïÎ•†Ï∂îÏ†ï NLG\n",
        "=> Tokenizing\n",
        "image.png\n",
        "\n",
        "P(Ïéà|ÏïåÏûòÎî±Íπî) => 1\n",
        "P(?|ÏïåÏûòÎî±Íπî) => 1/0.~ => > 1,\n",
        "P(Ïéà|ÏïåÏûòÎî±Íπî) => P(ÏïåÏûòÎî±ÍπîÏéà)/P(ÏïåÏûòÎî±Íπî)\n",
        "P(Ïûò|Ïïå)*P(Îî±|ÏïåÏûò)*P(Íπî|ÏïåÏûòÎî±)*P(Ïéà|ÏïåÏûòÎî±Íπî)\n",
        "P(ÏïåÏûò)   P(ÏïåÏûòÎî±)   P(ÏïåÏûòÎî±Íπî)       P(Ï†ÑÏ≤¥Ï°∞Ìï©)\n",
        "------ X ------- X -------    => ----------\n",
        "P(Ïïå)     P(ÏïåÏûò)   P (ÏïåÏûòÎî±)        P(Ï≤´Í∏ÄÏûê)\n",
        "=> Cohesion Score => Î∂ÑÎ™® = 1\n",
        "find(tuple('Î†à'), ngram1)\n",
        "[(('Î†à',), 130)]\n",
        "find(tuple('Î†àÏù¥'), ngram2)\n",
        "[(('Î†à', 'Ïù¥'), 81)]\n",
        "find(tuple('Î†àÏù¥'), ngram3)\n",
        "[(('Î†à', 'Ïù¥', ' '), 46),\n",
        " (('Î†à', 'Ïù¥', \"'\"), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ïùò'), 9),\n",
        " (('Î†à', 'Ïù¥', 'Îùº'), 1),\n",
        " (('Î†à', 'Ïù¥', 'Îäî'), 6),\n",
        " (('Î†à', 'Ïù¥', 'ÌÅ¨'), 1),\n",
        " (('Î†à', 'Ïù¥', 'ÏÖò'), 3),\n",
        " (('Î†à', 'Ïù¥', 'Î•º'), 3),\n",
        " (('Î†à', 'Ïù¥', 'Í∞Ä'), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ï†Ä'), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ïóê'), 2),\n",
        " (('Î†à', 'Ïù¥', ','), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ïä§'), 1),\n",
        " (('Î†à', 'Ïù¥', 'ÏÇ∞'), 1),\n",
        " (('Î†à', 'Ïù¥', '.'), 1),\n",
        " (('Î†à', 'Ïù¥', '['), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ìïô'), 1),\n",
        " (('Î†à', 'Ïù¥', 'Ïãú'), 1)]\n",
        "81/130, (46/81)*(1/81)*(9/81)\n",
        "(0.6230769230769231, 0.0007790140391877931)\n",
        "[Ïñ¥Í∑º/Ïñ¥Í∞Ñ ÌòïÌÉúÏÜå]+ÌòïÌÉúÏÜå(ÌòïÏãù/ÏùòÏ°¥)\n",
        "*     |\n",
        "  ****|\n",
        "      |***\n",
        "ÎåÄ ÌÜµ Î†π  Ïù¥\n",
        "        ÍªòÏÑú.."
      ]
    }
  ]
}