{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuqJAR5wJOg+pRic5/RkC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz75Ez4dXl16"
      },
      "outputs": [],
      "source": [
        "Preprocessing\n",
        "=> 모델링(X -> Model -> Y)\n",
        "   X -> encoding(X)\n",
        "   Categorical -> Numerical(1,2,3,4,...); i2w, w2i\n",
        "   NN: Orthogonal = Independent => One-hot, Layer\n",
        "   CountVectorize; 1:10 => Scale => 내일\n",
        "   TfidfVectorize; 1:w => [-,+], [0,+] => 내일\n",
        "   => Text, Independent(A,B,C / A,B,D / C,B,A => Dependency X), BoW\n",
        "   ; Occam's Razor => Feature selection\n",
        "   ; X => Features ?\n",
        "=> Feature Extraction = Tokenizing\n",
        "   문장, 토큰(단어=어절, 정규식, 특수기호집합, 음절, N-gram, 응집력, 불확실성, ...)\n",
        "           => word_extraction, stem+ming, lemm+tization, morpheme a => pos\n",
        "   entropy(X) => -p(앞) log p(앞) + -p(뒷) log p(뒷)\n",
        "               = -sum(p log p)\n",
        "                  sum(p(사건=앞면), p(사건=뒷면))\n",
        "               = AB?|AB => sum(p(AB[의,는,..]|AB))\n",
        "                 -p(앞=0) log p(앞=0) + -p(뒷=1) log p(뒷=1)\n",
        "                  0*-inf + -1*0 = 0\n",
        "| *\n",
        "|   *\n",
        "|      *\n",
        "0---------1\n",
        "=> Feature Normalization\n",
        "   대소문자일치, ', ., 표현(대문자), 약어->풀어\n",
        "   불용어(stopwords) => to be or not to be, state of the art\n",
        "   형태소사전 -> POS(품사), 형태소+품사\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "ma = Komoran()\n",
        "\n",
        "stopwords = '은 는 이 가 을 를 에게 께서'.split()\n",
        "\n",
        "data = '''\n",
        "어머니는 자장면이 싫다고 하셨어.\n",
        "엄마는 짜장면이 넘나 싫다고 했어.\n",
        "우리 엄마는 짜장이 극혐이라 했었어.\n",
        "'''\n",
        "unique = list()\n",
        "pos = list()\n",
        "for s in sent_tokenize(data):\n",
        "#     print(' '.join([t for t in ma.morphs(s)\n",
        "#                     if t not in stopwords and \\\n",
        "#                     len(t) > 1]))\n",
        "    unique.extend(ma.morphs(s))\n",
        "    pos.extend([t[1][0] for t in ma.pos(s)])\n",
        "    print(' '.join([t[0] for t in ma.pos(s)\n",
        "                    if t[1][0] not in ['J', 'E']]))\n",
        "\n",
        "# len(list(set(unique)))\n",
        "# 어머니, 엄마, 짜장면, 자장면, 싫다\n",
        "어머니 자장면 다고\n",
        "엄마 짜장면 다고\n",
        "우리 엄마 장이 았었\n",
        "from collections import Counter\n",
        "sum(Counter(pos).values()), Counter(pos)\n",
        "(37, Counter({'N': 9, 'J': 5, 'V': 8, 'E': 12, 'S': 3}))\n",
        "17/37\n",
        "0.4594594594594595\n",
        "ma.tagset['JX'], ma.tagset['JKS'], ma.tagset['EC'], ma.tagset['EP']\n",
        "('보조사', '주격 조사', '연결 어미', '선어말어미')\n",
        "한 사람\n",
        "한/사람, '한 ', ' 한'\n",
        "1  2\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "\n",
        "# \\b시|씨|ㅆ(.*)발\\b\n",
        "stopwords = ['시발', '씨발', '시이발']\n",
        "p1 = re.compile(f'[{re.escape(punctuation)}]')\n",
        "p2 = re.compile(r'\\B\\W\\B')\n",
        "\n",
        "s = input()\n",
        "' '.join([t if t not in stopwords else '*'*len(t)\n",
        "          for t in word_tokenize(p2.sub(' ', p1.sub('', s)))])\n",
        "시@#%@#발\n",
        "'**'\n",
        "선물 포장 이라고 기재해 주세요\n",
        "선물포장 => X,\n",
        "# BPE =>\n",
        "data = '''\n",
        "시발 시발 씨발 시이발 시~@!#$발 시부렁\n",
        "'''\n",
        "\n",
        "data = Counter(data.split())\n",
        "\n",
        "stopwords = dict()\n",
        "for k,v in data.items():\n",
        "    stopwords[('<w>',) + tuple(k)+('</w>',)] = v\n",
        "from collections import defaultdict\n",
        "defaultdict(lambda:1)['key']\n",
        "1\n",
        "from collections import defaultdict\n",
        "\n",
        "def findPair(kv):\n",
        "    pairs = defaultdict(lambda:0)\n",
        "\n",
        "    for k, v in kv.items():\n",
        "        for i in range(len(k)-1):\n",
        "            pairs[' '.join(k[i:i+2])] += v\n",
        "    return pairs\n",
        "p = findPair(stopwords)\n",
        "max(p, key=p.get)\n",
        "'<w> 시'\n",
        "keys = list()\n",
        "for _ in range(3):\n",
        "    pairs = findPair(stopwords)\n",
        "    maxkey = max(pairs, key=pairs.get)\n",
        "    keys.append(maxkey)\n",
        "    temp = dict()\n",
        "    for k,v in stopwords.items():\n",
        "        nk = tuple(re.sub(maxkey, maxkey.replace(' ', ''),\n",
        "                          ' '.join(k)).split())\n",
        "        temp[nk] = v\n",
        "    stopwords = temp\n",
        "a = list(map(lambda k:re.sub('</?w>', r'\\b', k), keys))\n",
        "# 1:<w>글자, 2:글자</w>, 3:<w>글자</w>\n",
        "#             <w>시, <w>씨, 발</w>\n",
        "# 앞뒤 어느 한쪽이\\b 아니면, 1 or 2, <w>시발</w>, <w>씨발</w>\n",
        "문장 = [1.어절] 어절 어절\n",
        "      [2.[형태소]+형태소] 형태소+형태소\n",
        "      [3.[음절]음절음절] 음절음절음절 음절음절음절\n",
        "      [4.[초성;자음+중성;모음+중성;자음]]\n",
        "1 00000 00000 00000 = 16\n",
        "    초     중     종    => 조합형()\n",
        "가->나 or 각, 가 -> 하 + => 확장완성형코드(CP949, EUC-KR)\n",
        "Unicode <- 음절\n",
        "ord('가'), ord('나'), ord('나')-ord('가'), ord('개')-ord('가')\n",
        "(44032, 45208, 1176, 28)\n",
        "# print([chr(ord('가')+(i)) for i in range(28)])\n",
        "# print([chr(ord('가')+(i*28)) for i in range(21)])\n",
        "# print([chr(ord('가')+(i*21*28)) for i in range(19)])\n",
        "['가', '개', '갸', '걔', '거', '게', '겨', '계', '고', '과', '괘', '괴', '교', '구', '궈', '궤', '귀', '규', '그', '긔', '기']\n",
        "cho =  ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ',\n",
        "        'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ',\n",
        "        'ㅌ', 'ㅍ', 'ㅎ']\n",
        "jung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ',\n",
        "        'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ',\n",
        "        'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
        "jong = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ',\n",
        "        'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ',\n",
        "        'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ',\n",
        "        'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
        "len(cho), len(jung), len(jong), 19*21*28\n",
        "(19, 21, 28, 11172)\n",
        "def chr2tri(c):\n",
        "    # 1음절 가정\n",
        "    if re.match('[가-힣]', c):\n",
        "        base = divmod(ord(c)-ord('가'), (21*28))\n",
        "        c1 = base[0]\n",
        "        c2 = divmod(base[-1], 28)[0]\n",
        "        c3 = divmod(base[-1], 28)[-1]\n",
        "        return cho[c1]+jung[c2]+jong[c3]\n",
        "    else:\n",
        "        return c\n",
        "\n",
        "def tri2char(c):\n",
        "    # 3음절 가정\n",
        "    if re.match(r'^[ ㄱ-ㅎㅏ-ㅣ]{3}$', c):\n",
        "        r = ord('가') + cho.index(c[0])*21*28\n",
        "        r += jung.index(c[1])*28\n",
        "        r += jong.index(c[2])\n",
        "        return chr(r)\n",
        "    else:\n",
        "        return c\n",
        "tri2char('ㄱㅏ '), tri2char(chr2tri('걁'))\n",
        "('가', '걁')\n",
        "형태소 => 1음절 단어 => 초중종\n",
        "안녕하세요 => 아녕하세요\n",
        "ma.pos('안녕하세요'), ma.pos('아녕하세요')\n",
        "([('안녕하세요', 'NNP')], [('아녕하세요', 'NA')])\n",
        "ma.tagset['NA']\n",
        "'분석불능범주'\n",
        "# Edit Distance\n",
        "# Hamming, Leven\n",
        "def hamming(s1, s2):\n",
        "    if len(s1) != len(s2):\n",
        "        return '분석불가'\n",
        "\n",
        "    return sum([c1 != c2 for c1, c2 in zip(s1, s2)])\n",
        "hamming('안녕하세요', '안ㄴ녕하세')\n",
        "4\n",
        "#   안 ㄴ 하 세 요 B\n",
        "# 안 0\n",
        "# 녕   1\n",
        "# 하\n",
        "# 세\n",
        "# 요           ? = 거리\n",
        "# A\n",
        "def leven(s1, s2):\n",
        "    if len(s1) == 0:\n",
        "        return len(s2)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    if s1[0] == s2[0]:\n",
        "        return leven(s1[1:], s2[1:])\n",
        "    else:\n",
        "        v1 = .1 + leven(s1[1:], s2[1:]) # 교체\n",
        "        v2 = 3 + leven(s1, s2[1:]) # 추가\n",
        "        v3 = 2 + leven(s1[1:], s2) # 삭제\n",
        "        return min((v1, v2, v3))\n",
        "\n",
        "leven('안녕하세요', '아녕하세요'), leven('한성대', '한세대')\n",
        "(0.1, 0.1)\n",
        "s1 = ''.join(list(map(lambda c:chr2tri(c), '안녕하세요')))\n",
        "s2 = ''.join(list(map(lambda c:chr2tri(c), '아녕하세요')))\n",
        "leven(s1, s2)\n",
        "0.1\n",
        "s1 = ''.join(list(map(lambda c:chr2tri(c), '한성대')))\n",
        "s2 = ''.join(list(map(lambda c:chr2tri(c), '한세대')))\n",
        "leven(s1, s2)\n",
        "0.2\n",
        "[Preprocessing] + 언어학/국문학(형태소분석) + 통계(Entropy, P, Perplexity)\n",
        "Tokenizing + Normalization(형식/의존형태소=>ma, 품사=>pos, 길이)\n",
        "           + Stopwords\n",
        "           + Edit ditance\n",
        "=> 좋은 Token 찾고, 정제\n",
        "https://www.cs.virginia.edu/~hw5x/Course/IR2021-Spring/"
      ]
    }
  ]
}