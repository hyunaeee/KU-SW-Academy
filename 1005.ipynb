{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk2zAijGgZUW8ycoPX/Ane",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ61gNmOW2Xi"
      },
      "outputs": [],
      "source": [
        "문서 = 문단 = 문장 => sent_tokenize\n",
        "str.splitlines() !=  sent_tokenize\n",
        "\\n => split          문장(구두점) => split\n",
        "                     .!? + whitespace => split\n",
        "                     인용문 => split\n",
        "=> 문어체, 문법 준수\n",
        "\n",
        "tokenizing => word_tokenize, regexp_tokenize, TweetTokenize\n",
        "              penntree, brown, ...\n",
        "              Morpheme analyzer + Entropy + Perplexity + BPE + NGRAM\n",
        "\n",
        "nltk.tokenize.* => punk, lang='english', korean X\n",
        "str.split()    !=     nltk.tokenize.*\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "emma = gutenberg.open(gutenberg.fileids()[0]).read()\n",
        "from nltk.tokenize import *\n",
        "len(emma.splitlines()), len(sent_tokenize(emma))\n",
        "(16823, 7456)\n",
        "len(emma.split()), len(word_tokenize(emma)), \\\n",
        "len(set(emma.split())), len(set(word_tokenize(emma)))\n",
        "split => 어절 단위\n",
        "tokenize => 어절 + 구두점..\n",
        "(158167, 191776, 17409, 8409)\n",
        "len(set(regexp_tokenize(emma, r'\\b\\w+\\b'))),\\\n",
        "len(regexp_tokenize(emma, r'\\b\\w+\\b'))\n",
        "(7723, 161983)\n",
        "tokenizer = TweetTokenizer()\n",
        "len(set(tokenizer.tokenize(emma))),\\\n",
        "len(tokenizer.tokenize(emma))\n",
        "(8938, 193228)\n",
        "from nltk.text import Text\n",
        "emmaObj = Text(word_tokenize(emma))\n",
        "emmaObj.vocab().B(), emmaObj.vocab().N()\n",
        "=> Count\n",
        "(8409, 191776)\n",
        "emmaObj.vocab().freq(','), emmaObj.vocab().get(',')/emmaObj.vocab().N()\n",
        "(0.06265643250458869, 0.06265643250458869)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,51), [1/i for i in range(1,51)])\n",
        "[<matplotlib.lines.Line2D at 0x2981ef070>]\n",
        "\n",
        "maxfreq = emmaObj.vocab().get(',')\n",
        "N = 51\n",
        "plt.plot(range(1,N), [1/i for i in range(1,N)])\n",
        "plt.plot(range(1,N), [t[1]/maxfreq\n",
        "                      for t in emmaObj.vocab().most_common(N-1)])\n",
        "[<matplotlib.lines.Line2D at 0x29b628730>]\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "len(set(pos_tag(word_tokenize(emma)))), \\\n",
        "len(pos_tag(word_tokenize(emma)))\n",
        "(10978, 191776)\n",
        "emmaPos = Text(pos_tag(word_tokenize(emma)))\n",
        "N = 51\n",
        "plt.plot(range(1,N), [1/i for i in range(1,N)])\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/maxfreq for t in emmaObj.vocab().most_common(N-1)],\n",
        "         'r-')\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/maxfreq for t in emmaPos.vocab().most_common(N-1)],\n",
        "         'b-')\n",
        "[<matplotlib.lines.Line2D at 0x29c7f63a0>]\n",
        "\n",
        "penn = treebank.TreebankWordTokenizer().tokenize(emma)\n",
        "len(set(penn)), len(penn)\n",
        "(9852, 185326)\n",
        "from nltk.help import upenn_tagset\n",
        "emmaPenn = Text(penn)\n",
        "N = 51\n",
        "plt.plot(range(1,N), [1/i for i in range(1,N)])\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/maxfreq for t in emmaObj.vocab().most_common(N-1)],\n",
        "         'r-')\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/maxfreq for t in emmaPos.vocab().most_common(N-1)],\n",
        "         'b-')\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/maxfreq for t in emmaPenn.vocab().most_common(N-1)],\n",
        "         'k-')\n",
        "[<matplotlib.lines.Line2D at 0x2a8100100>]\n",
        "\n",
        "sorted(emmaObj.vocab(), key=emmaObj.vocab().get)[:10]\n",
        "['Austen',\n",
        " '1816',\n",
        " 'twenty-one',\n",
        " 'vex',\n",
        " 'indistinct',\n",
        " 'caresses',\n",
        " 'nominal',\n",
        " 'mildness',\n",
        " 'impose',\n",
        " 'esteeming']\n",
        "emmaObj.vocab().get('austen')\n",
        "sorted(emmaObj.vocab(), key=emmaObj.vocab().get, reverse=True)[:10]\n",
        "[',', '.', 'to', 'the', 'and', 'of', 'I', '--', 'a', \"''\"]\n",
        "sumof = 0.0\n",
        "for t in sorted(emmaObj.vocab(), key=emmaObj.vocab().get,\n",
        "                reverse=True)[:50]:\n",
        "    sumof += emmaObj.vocab().freq(t)\n",
        "sumof\n",
        "0.515971758718505\n",
        "emmaObj.vocab().B()\n",
        "8409\n",
        "sumof = 0.0\n",
        "for t in sorted(emmaObj.vocab(), key=emmaObj.vocab().get)[:2000]:\n",
        "    sumof += emmaObj.vocab().freq(t)\n",
        "sumof\n",
        "0.010428833639245546\n",
        "emmaObj.collocations() => NGRAM\n",
        "Mr. Knightley; Mrs. Weston; Frank Churchill; Mr. Elton; Miss\n",
        "Woodhouse; Miss Bates; Mrs. Elton; Miss Fairfax; Mr. Weston; Jane\n",
        "Fairfax; every thing; Mr. Woodhouse; every body; young man; great\n",
        "deal; dare say; Maple Grove; Mrs. Goddard; John Knightley; Miss Smith\n",
        "emmaObj.concordance('Emma')\n",
        "Displaying 25 of 855 matches:\n",
        "[ Emma by Jane Austen 1816 ] VOLUME I CHAPT\n",
        "ane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and\n",
        "both daughters , but particularly of Emma . Between _them_ it was more the int\n",
        " friend very mutually attached , and Emma doing just what she liked ; highly e\n",
        "r own . The real evils , indeed , of Emma 's situation were the power of havin\n",
        "ding-day of this beloved friend that Emma first sat in mournful thought of any\n",
        "ing only half a mile from them ; but Emma was aware that great must be the dif\n",
        "y . It was a melancholy change ; and Emma could not but sigh over it , and wis\n",
        " the rest of her life at Hartfield . Emma smiled and chatted as cheerfully as\n",
        "able to tell her how we all are . '' Emma spared no exertions to maintain this\n",
        " ' I have a great regard for you and Emma ; but when it comes to the question\n",
        "ful , troublesome creature ! '' said Emma playfully . `` That is what you have\n",
        "e few people who could see faults in Emma Woodhouse , and the only one who eve\n",
        "is was not particularly agreeable to Emma herself , she knew it would be so mu\n",
        "g thought perfect by every body . `` Emma knows I never flatter her , '' said\n",
        "t be a gainer . '' `` Well , '' said Emma , willing to let it pass -- '' you w\n",
        "re of meeting every day . '' `` Dear Emma bears every thing so well , '' said\n",
        "ss her more than she thinks for . '' Emma turned away her head , divided betwe\n",
        "nd smiles . `` It is impossible that Emma should not miss such a companion , '\n",
        "en one matter of joy to me , '' said Emma , '' and a very considerable one --\n",
        "od to them , by interference . '' `` Emma never thinks of herself , if she can\n",
        "etter thing . Invite him to dinner , Emma , and help him to the best of the fi\n",
        " could not think , without pain , of Emma 's losing a single pleasure , or suf\n",
        " of her companionableness : but dear Emma was of no feeble character ; she was\n",
        ", was so just and so apparent , that Emma , well as she knew her father , was\n",
        "emmaObj.dispersion_plot(['Emma', 'Elton', 'Weston'])\n",
        "\n",
        "emmaObj.similar('Emma'), \\\n",
        "emmaObj.similar('Weston'), \\\n",
        "emmaObj.similar('Elton')\n",
        "she it he i weston you her harriet elton him me knightley jane the\n",
        "that and all there they them\n",
        "it emma she he i her you harriet elton that him knightley and them\n",
        "herself there me all what highbury\n",
        "it he her she emma weston you knightley harriet him i the all there\n",
        "and highbury me his them what\n",
        "(None, None, None)\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('waied')\n",
        "'wai'\n",
        "emmaObj.vocab().get('')\n",
        "5\n",
        "from nltk.collocations import *\n",
        "bigram = BigramCollocationFinder(word_tokenize(emma), )\n",
        "---------------------------------------------------------------------------\n",
        "TypeError                                 Traceback (most recent call last)\n",
        "Cell In [95], line 1\n",
        "----> 1 bigram = BigramCollocationFinder(word_tokenize(emma))\n",
        "\n",
        "TypeError: __init__() missing 1 required positional argument: 'bigram_fd'\n",
        "bigram = BigramCollocationFinder.from_words(word_tokenize(emma))\n",
        "bigram.nbest(BigramAssocMeasures().chi_sq, 10)\n",
        "[('&', 'c.'),\n",
        " ('10,000', 'l.'),\n",
        " ('26th', 'ult.'),\n",
        " ('Abominable', 'scoundrel'),\n",
        " ('Agricultural', 'Reports'),\n",
        " ('Austen', '1816'),\n",
        " ('Baronne', \"d'Almane\"),\n",
        " ('Candles', 'everywhere.'),\n",
        " ('Clayton', 'Park'),\n",
        " ('Comtesse', \"d'Ostalis\")]\n",
        "# Zipf\n",
        "단어의 빈도로 나열 = 순위의 역\n",
        "1 .5 .25 ...                1 .5\n",
        "고빈도      중빈도      저빈도\n",
        "1. 고빈도; 소수의 몇개의 단어가 전체의 거의 대부분을 차지 ? (X)\n",
        "3. 저빈도; 유니크한 단어의 25%정도가 전체의 1% 내외를 차지 ? (X)\n",
        "2. 중빈도; 중요한\n",
        "\n",
        "빈도, 비율, 길이, Stem\n",
        "=> Text, collocation(N-gram), correlation(co-occurence) <- metrics\n",
        "t = list()\n",
        "\n",
        "for file in gutenberg.fileids()[:10]:\n",
        "    doc = gutenberg.open(file).read()\n",
        "    t.append(Text(word_tokenize(doc)))\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "uniq = list()\n",
        "total = list()\n",
        "\n",
        "objs = FreqDist()\n",
        "\n",
        "for obj in t:\n",
        "    objs += obj.vocab()\n",
        "    uniq.append(objs.B())\n",
        "    total.append(objs.N())\n",
        "k = 65 #10-100\n",
        "b = .42 #.4-.6\n",
        "plt.plot(range(5), uniq)\n",
        "plt.plot(range(5), list(map(lambda n:k*n**b, total)))\n",
        "[<matplotlib.lines.Line2D at 0x2b5fb64f0>]\n",
        "\n",
        "k*objs.N()**b\n",
        "26740.00206687489\n",
        "objs.B(), objs.N()\n",
        "(34966, 1676487)\n",
        "plt.plot(range(10), uniq)\n",
        "plt.plot(range(10), list(map(lambda n:k*n**b, total)))\n",
        "[<matplotlib.lines.Line2D at 0x2c34da460>]\n",
        "\n",
        "Heaps\n",
        "문서 내 전체 단어의 빈도는 => 특정 법칙에 의해 유니크한 단어의 수와 비례한다\n",
        "k = 65, b = .42\n",
        "=> 데이터셋(코퍼스;말뭉치)의 증가에 / 유니크한수(features)가 결정된다\n",
        "=> feature selection / model size 가이드\n",
        "from konlpy.corpus import kobill, kolaw\n",
        "from konlpy.tag import Hannanum, Komoran, Kkma, Okt\n",
        "ma = {'hannanum':Hannanum(), 'komoran':Komoran(),\n",
        "      'kkma':Kkma(), 'okt':Okt()}\n",
        "data = '실제로 국내 필러 시장 규모는 지난 2020년 1200억 원에서 연평균 9.1%로 성장해 2026년에는 약 2023억 원에 이를 것으로 전망된다. 게다가 필러는 비교적 간단한 시술로 주기적으로 받는 사람이 많다. 그런데 자주 받아도 괜찮은 걸까?'\n",
        "data = '들어가셨다 들어가다 들어가시다 들어가셨다'\n",
        "for k,v in ma.items():\n",
        "    print(k)\n",
        "    print(v.pos(data))\n",
        "    print(len(v.pos(data)))\n",
        "hannanum\n",
        "[('들', 'P'), ('어', 'E'), ('가', 'P'), ('셨다', 'E'), ('들', 'P'), ('어', 'E'), ('가', 'P'), ('아', 'E'), ('들', 'P'), ('어', 'E'), ('가', 'P'), ('시다', 'E'), ('들', 'P'), ('어', 'E'), ('가', 'P'), ('셨다', 'E')]\n",
        "16\n",
        "komoran\n",
        "[('들어가', 'VV'), ('시', 'EP'), ('었', 'EP'), ('다', 'EC'), ('들어가', 'VV'), ('다', 'EC'), ('들어가', 'VV'), ('시', 'EP'), ('다', 'EC'), ('들어가', 'VV'), ('시', 'EP'), ('었', 'EP'), ('다', 'EC')]\n",
        "13\n",
        "kkma\n",
        "[('들어가', 'VV'), ('시', 'EPH'), ('었', 'EPT'), ('다', 'ECS'), ('들어가', 'VV'), ('다', 'ECS'), ('들어가', 'VV'), ('시', 'EPH'), ('다', 'ECS'), ('들어가', 'VV'), ('시', 'EPH'), ('었', 'EPT'), ('다', 'EFN')]\n",
        "13\n",
        "okt\n",
        "[('들어가셨다', 'Verb'), ('들어가다', 'Verb'), ('들어가시다', 'Verb'), ('들어가셨다', 'Verb')]\n",
        "4\n",
        "ma['komoran'].tagset['VV']\n",
        "'동사'\n",
        "ma['komoran'].tagset\n",
        "{'EC': '연결 어미',\n",
        " 'EF': '종결 어미',\n",
        " 'EP': '선어말어미',\n",
        " 'ETM': '관형형 전성 어미',\n",
        " 'ETN': '명사형 전성 어미',\n",
        " 'IC': '감탄사',\n",
        " 'JC': '접속 조사',\n",
        " 'JKB': '부사격 조사',\n",
        " 'JKC': '보격 조사',\n",
        " 'JKG': '관형격 조사',\n",
        " 'JKO': '목적격 조사',\n",
        " 'JKQ': '인용격 조사',\n",
        " 'JKS': '주격 조사',\n",
        " 'JKV': '호격 조사',\n",
        " 'JX': '보조사',\n",
        " 'MAG': '일반 부사',\n",
        " 'MAJ': '접속 부사',\n",
        " 'MM': '관형사',\n",
        " 'NA': '분석불능범주',\n",
        " 'NF': '명사추정범주',\n",
        " 'NNB': '의존 명사',\n",
        " 'NNG': '일반 명사',\n",
        " 'NNP': '고유 명사',\n",
        " 'NP': '대명사',\n",
        " 'NR': '수사',\n",
        " 'NV': '용언추정범주',\n",
        " 'SE': '줄임표',\n",
        " 'SF': '마침표, 물음표, 느낌표',\n",
        " 'SH': '한자',\n",
        " 'SL': '외국어',\n",
        " 'SN': '숫자',\n",
        " 'SO': '붙임표(물결,숨김,빠짐)',\n",
        " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
        " 'SS': '따옴표,괄호표,줄표',\n",
        " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
        " 'VA': '형용사',\n",
        " 'VCN': '부정 지정사',\n",
        " 'VCP': '긍정 지정사',\n",
        " 'VV': '동사',\n",
        " 'VX': '보조 용언',\n",
        " 'XPN': '체언 접두사',\n",
        " 'XR': '어근',\n",
        " 'XSA': '형용사 파생 접미사',\n",
        " 'XSN': '명사파생 접미사',\n",
        " 'XSV': '동사 파생 접미사'}\n",
        "# 나는 너랑 밥먹으러 가고싶어 X\n",
        "# 밥먹을래\n",
        "# 너 밥먹을래\n",
        "# 밥? <- sentence\n",
        "\n",
        "ma['komoran'].nouns('실제로 국내 필러 시장 규모는 지난 2020년 1200억 원에서 연평균 9.1%로 성장해 2026년에는 약 2023억 원에 이를 것으로 전망된다. 게다가 필러는 비교적 간단한 시술로 주기적으로 받는 사람이 많다. 그런데 자주 받아도 괜찮은 걸까?')\n",
        "['국내',\n",
        " '필러',\n",
        " '시장',\n",
        " '규모',\n",
        " '년',\n",
        " '원',\n",
        " '연평균',\n",
        " '성장',\n",
        " '년',\n",
        " '원',\n",
        " '것',\n",
        " '전망',\n",
        " '필러',\n",
        " '비교',\n",
        " '시술',\n",
        " '주기',\n",
        " '사람']\n",
        "law = kolaw.open(kolaw.fileids()[0]).read()\n",
        "len(law), len(law.split()), len(set(law.split())), len(law.splitlines())\n",
        "(18884, 4178, 2029, 356)\n",
        "len(word_tokenize(law)), len(set(word_tokenize(law)))\n",
        "(4640, 2023)\n",
        "len(regexp_tokenize(law, r'\\b\\w+\\b')), \\\n",
        "len(set(regexp_tokenize(law, r'\\b\\w+\\b')))\n",
        "(4325, 2085)\n",
        "len(tokenizer.tokenize(law)), \\\n",
        "len(set(tokenizer.tokenize(law)))\n",
        "(5099, 2007)\n",
        "len(sent_tokenize(law))\n",
        "357\n",
        "len(ma['komoran'].morphs(law)), len(set(ma['komoran'].morphs(law)))\n",
        "(9780, 1214)\n",
        "t1 = Text(word_tokenize(law))\n",
        "t2 = Text(ma['komoran'].morphs(law))\n",
        "t1.vocab().B(), t2.vocab().B()\n",
        "(2023, 1214)\n",
        "t1.vocab().N(), t2.vocab().N()\n",
        "(4640, 9780)\n",
        "list(zip(t1.vocab().most_common(10), t2.vocab().most_common(10)))\n",
        "[(('.', 357), ('하', 472)),\n",
        " ((',', 101), ('의', 387)),\n",
        " (('수', 87), ('.', 360)),\n",
        " (('①', 75), ('에', 330)),\n",
        " (('또는', 70), ('는', 287)),\n",
        " (('의하여', 66), ('ㄴ다', 243)),\n",
        " (('법률이', 57), ('ㄴ', 236)),\n",
        " (('있다', 57), ('이', 230)),\n",
        " (('한다', 56), ('을', 225)),\n",
        " (('정하는', 50), ('은', 199))]\n",
        "N = 51\n",
        "plt.plot(range(1,N), [1/i for i in range(1,N)])\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/357 for t in t1.vocab().most_common(N-1)],\n",
        "         'r-')\n",
        "plt.plot(range(1,N),\n",
        "         [t[1]/472 for t in t2.vocab().most_common(N-1)],\n",
        "         'b-')\n",
        "[<matplotlib.lines.Line2D at 0x2aefcc520>]\n",
        "\n",
        "sumof = 0.0\n",
        "for k in sorted(t2.vocab(), key=t2.vocab().get, reverse=True)[:30]:\n",
        "    sumof += t2.vocab().freq(k)\n",
        "sumof\n",
        "0.5221881390593046\n",
        "sumof = 0.0\n",
        "for k in sorted(t2.vocab(), key=t2.vocab().get)[:800]:\n",
        "    sumof += t2.vocab().freq(k)\n",
        "sumof\n",
        "0.10132924335378457\n",
        "list(filter(lambda t:len(t)>1,\n",
        "       sorted(t2.vocab(), key=t2.vocab().get, reverse=True)[30:-800]))\n",
        "316\n",
        "t2.vocab().B()\n",
        "1214\n",
        "total = 0\n",
        "for t in list(filter(lambda t:t.startswith('법률'),\n",
        "                     t1.vocab().keys())):\n",
        "    total += t1.vocab().get(t)\n",
        "total\n",
        "123\n",
        "import re\n",
        "ktokens = list() # [Text(문서1).vocab() = FreqDist, 문서2, ..., 문서10]\n",
        "kmorphs = list()\n",
        "\n",
        "ktokens.append(FreqDist()) # [0:-1;FreqDist() + 지금분석]\n",
        "kmorphs.append(FreqDist())\n",
        "\n",
        "for doc in kobill.fileids(): # 의안10\n",
        "    morphs = list()\n",
        "    corpus = kobill.open(doc).read() # n번째 의안 읽음 => 코퍼스\n",
        "    ktokens.append(ktokens[-1] + Text(word_tokenize(corpus)).vocab())\n",
        "    for s in sent_tokenize(re.sub(r'\\s{2,}', ' ', corpus)):\n",
        "        if len(s) > 1:\n",
        "            morphs.extend(ma['komoran'].morphs(s))\n",
        "    kmorphs.append(kmorphs[-1] + Text(morphs).vocab())\n",
        "#     [s1 => []\n",
        "#     s2 => []\n",
        "#     s3 => []\n",
        "#     ...]\n",
        "#     [s1 + s2 + s3, ...]\n",
        "for k in ktokens:\n",
        "    print(k.N(), k.B())\n",
        "0 0\n",
        "2443 768\n",
        "3351 1092\n",
        "3857 1244\n",
        "4133 1330\n",
        "5257 1726\n",
        "6373 1732\n",
        "7462 1791\n",
        "8860 1874\n",
        "11392 2316\n",
        "12338 2479\n",
        "k = 65 #10-100\n",
        "b = .42 #.4-.6\n",
        "k = 30 #10-100\n",
        "b = .54 #.4-.6\n",
        "plt.plot(range(10), [t.B() for t in ktokens[1:]], 'r-')\n",
        "plt.plot(range(10), list(map(lambda n:k*n.B()**b, ktokens[1:])))\n",
        "[<matplotlib.lines.Line2D at 0x2b6c97970>]\n",
        "\n",
        "k = 25 #10-100\n",
        "b = .54\n",
        "plt.plot(range(10), [t.B() for t in kmorphs[1:]], 'r-')\n",
        "plt.plot(range(10), list(map(lambda n:k*n.B()**b, kmorphs[1:])))\n",
        "[<matplotlib.lines.Line2D at 0x2c2277ee0>]\n",
        "\n",
        "lawText = Text(ma['komoran'].morphs(law))\n",
        "# lawText.collocation_list()\n",
        "lawText.collocations()\n",
        "헌법재판소 재판관\n",
        "lawText.concordance('국민')\n",
        "Displaying 25 of 67 matches:\n",
        "대한민국 헌법 유구 하 ㄴ 역사 와 전통 에 빛나 는 우리 대하 ㄴ 국민 은 3 · 1 운동 으로 건립 되 ㄴ 대한민국 임시 정부 의 법통\n",
        " 에 따르 는 책임 과 의무 를 완수 하 게 하 아 , 안 으로 는 국민 생활 의 균등 하 ㄴ 향상 을 기하 고 밖 으로 는 항구 적 이 ㄴ\n",
        " 에 걸치 어 개정 되 ㄴ 헌법 을 이제 국회 의 의결 을 거치 어 국민 투표 에 의하 아 개정 하 ㄴ다 . 제 1 장 총 강 제 1 조 ①\n",
        " 조 ① 대한민국 은 민주공화국 이 다 . ② 대한민국 의 주권 은 국민 에게 있 고 , 모든 권력 은 국민 으로부터 나오 ㄴ다 . 제 2\n",
        "다 . ② 대한민국 의 주권 은 국민 에게 있 고 , 모든 권력 은 국민 으로부터 나오 ㄴ다 . 제 2 조 ① 대한민국 의 국민 이 되 는\n",
        "든 권력 은 국민 으로부터 나오 ㄴ다 . 제 2 조 ① 대한민국 의 국민 이 되 는 요건 은 법률 로 정하 ㄴ다 . ② 국가 는 법률 이 정\n",
        "에 의하 아 그 지위 가 보장 되 ㄴ다 . 제 7 조 ① 공무원 은 국민 전체 에 대하 ㄴ 봉사자 이 며 , 국민 에 대하 아 책임 을 지\n",
        " 제 7 조 ① 공무원 은 국민 전체 에 대하 ㄴ 봉사자 이 며 , 국민 에 대하 아 책임 을 지 ㄴ다 . ② 공무원 의 신분 과 정치 적\n",
        "정당 은 그 목적 · 조직 과 활동 이 민주 적 이 어야 하 며 , 국민 의 정치 적 의사 형성 에 참여 하 는데 필요 하 ㄴ 조직 을 가지\n",
        " 아야 하 ㄴ다 . 제 2 장 국민의 권리와 의무 제 10 조 모든 국민 은 인간 으로서 의 존엄 과 가치 를 가지 며 , 행복 을 추구 하\n",
        " 고 이 를 보장 하 ㄹ 의무 를 지 ㄴ다 . 제 11 조 ① 모든 국민 은 법 앞 에 평등 하 다 . 누구 이 든지 성별 · 종교 또는 사\n",
        " ㄴ 특권 도 이 에 따르 지 아니하 ㄴ다 . 제 12 조 ① 모든 국민 은 신체의 자유 를 가지 ㄴ다 . 누구 이 든지 법률 에 의하 지\n",
        "벌 · 보안처분 또는 강제 노역 을 받 지 아니하 ㄴ다 . ② 모든 국민 은 고문 을 받 지 아니하 며 , 형사 상 자기 에게 불리 하 ㄴ\n",
        "나 이 를 이유 로 처벌 하 ㄹ 수 없 다 . 제 13 조 ① 모든 국민 은 행위 시 의 법률 에 의하 아 범죄 를 구성 하 지 아니하 는\n",
        "하 ㄴ 범죄 에 대하 아 거듭 처벌 받 지 아니하 ㄴ다 . ② 모든 국민 은 소급 입법 에 의하 아 참정권 의 제한 을 받 거나 재산권 을\n",
        "제한 을 받 거나 재산권 을 박탈 당하 지 아니하 ㄴ다 . ③ 모든 국민 은 자기 의 행위 가 아니 ㄴ 친족 의 행위 로 인하 아 불 이익한\n",
        "하 아 불 이익한 처우 를 받 지 아니하 ㄴ다 . 제 14 조 모든 국민 은 거주 · 이전 의 자유 를 가지 ㄴ다 . 제 15 조 모든 국민\n",
        "국민 은 거주 · 이전 의 자유 를 가지 ㄴ다 . 제 15 조 모든 국민 은 직업 선택 의 자유 를 가지 ㄴ다 . 제 16 조 모든 국민 은\n",
        "든 국민 은 직업 선택 의 자유 를 가지 ㄴ다 . 제 16 조 모든 국민 은 주거 의 자유 를 침해 받 지 아니하 ㄴ다 . 주거 에 대하 ㄴ\n",
        " 발부 하 ㄴ 영장 을 제시 하 아야 하 ㄴ다 . 제 17 조 모든 국민 은 사생활의 비밀과 자유 를 침해 받 지 아니하 ㄴ다 . 제 18\n",
        "활의 비밀과 자유 를 침해 받 지 아니하 ㄴ다 . 제 18 조 모든 국민 은 통신 의 비밀 을 침해 받 지 아니하 ㄴ다 . 제 19 조 모든\n",
        "은 통신 의 비밀 을 침해 받 지 아니하 ㄴ다 . 제 19 조 모든 국민 은 양심의 자유 를 가지 ㄴ다 . 제 20 조 ① 모든 국민 은 종\n",
        " 모든 국민 은 양심의 자유 를 가지 ㄴ다 . 제 20 조 ① 모든 국민 은 종교의 자유 를 가지 ㄴ다 . ② 국교 는 인정 되 지 아니하\n",
        "하 며 , 종교 와 정치 는 분리 되 ㄴ다 . 제 21 조 ① 모든 국민 은 언론 · 출판 의 자유 와 집회 · 결사의 자유 를 가지 ㄴ다\n",
        " 피해 의 배상 을 청구 하 ㄹ 수 있 다 . 제 22 조 ① 모든 국민 은 학문 과 예술 의 자유 를 가지 ㄴ다 . ② 저작자 · 발명가\n",
        "# lawText.similar('국민'),\n",
        "# lawText.similar('권력')\n",
        "lawText.similar('대통령')\n",
        "법률 국회의원 공무원 국무위원 법관 대법원장 국민 국회 국군 조약 정당 법원 국정 위원 대한민국 헌법 국가 정책 효력 발전\n",
        "bigram = BigramCollocationFinder.from_words(ma['komoran'].morphs(law))\n",
        "bigram.nbest(BigramAssocMeasures.chi_sq, 10)\n",
        "[('가부', '동수'),\n",
        " ('강제', '노역'),\n",
        " ('경자', '유전'),\n",
        " ('교전', '상태'),\n",
        " ('국립', '대학교'),\n",
        " ('군', '참모총장'),\n",
        " ('군용', '물'),\n",
        " ('궐위되거나', '사고'),\n",
        " ('내부', '규율'),\n",
        " ('더욱', '확고히')]\n",
        "아버지가 방에 들어가신다.\n",
        "아버지가 --?\n",
        "2-sequence\n",
        "아??\n",
        "ㅇ-?\n",
        "=> N-sequence => N-gram => LM\n",
        "P(?|given) = P(?,given) / P(given)\n",
        "2-gram\n",
        "ABCDE\n",
        "P(E|C)\n",
        "i => 4\n",
        "i-1 => 3\n",
        "i-(n-1) => i-1 => 3\n",
        "3-gram\n",
        "i-1 => 3\n",
        "i-(n-1) => i-2 => 2\n"
      ]
    }
  ]
}