{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZfDtAe/7Jm/VYVkSHdK+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1012.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev1IC82LXw4z"
      },
      "outputs": [],
      "source": [
        "1. Crawler; raw content\n",
        "2. Indexer; index term = token; controlled vocabulary\n",
        "3. Doc Anlayzer; Bag of Words(BoW)\n",
        "   => DocRerp.; doc1={t1,t2,t3,...,tn ㅌ CV}\n",
        "import re\n",
        "from os import listdir\n",
        "\n",
        "path = 'naver'\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(''.join(s[i:i+n]))\n",
        "    return rst\n",
        "\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "# 2. Indexer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "tokens = list()\n",
        "\n",
        "for file in fileids('naver'):\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        tokens.extend(regexp_tokenize(fp.read(), r'\\b\\w+\\b'))\n",
        "\n",
        "tokens = Counter(tokens)\n",
        "CV = list(tokens.keys())\n",
        "D = fileids('naver')\n",
        "DTM = [[0 for t in range(len(CV))] for d in range(len(D))]\n",
        "len(DTM), len(DTM[0])\n",
        "(87, 14498)\n",
        "for i, file in enumerate(D):\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        for t in regexp_tokenize(fp.read(), r'\\b\\w+\\b'):\n",
        "            if t in CV:\n",
        "                j = CV.index(t)\n",
        "                DTM[i][j] = 1\n",
        "sum([d[5]/len(D) for d in DTM])\n",
        "# DTM   단어1  단어2 ...\n",
        "# 문서1   n\n",
        "# 문서2   n\n",
        "# ...\n",
        "#      sum(단어1)/len(D) = 0\n",
        "Zipf's law 적용 X\n",
        "0.022988505747126436\n",
        "# 불리언 검색\n",
        "q = '대선 후보 인터뷰'\n",
        "\n",
        "rst = list()\n",
        "for wi in regexp_tokenize(q, r'\\b\\w+\\b'):\n",
        "    # Q = {wi ㅌ CV}\n",
        "    # TimeComplexity = O(|Q|*D*|D|)\n",
        "    # DTM = (87, 14498)\n",
        "    # 문서 전체(DTM 행이 => 문서 셋)\n",
        "    candi = list()\n",
        "    for i, d in enumerate(DTM):\n",
        "        # 단어 전체(각 행이 => 단어 셋)\n",
        "        for j, wj in enumerate(d):\n",
        "            if CV[j] == wi:\n",
        "                candi.append(i)\n",
        "                break\n",
        "\n",
        "    rst.append(candi)\n",
        "len(list(set(set(rst[0]).intersection(rst[1])).intersection(rst[2])))\n",
        "87\n",
        "TDM = [[0 for d in range(len(D))] for t in range(len(CV))]\n",
        "for i, file in enumerate(D):\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        for t in regexp_tokenize(fp.read(), r'\\b\\w+\\b'):\n",
        "            if t in CV:\n",
        "                j = CV.index(t)\n",
        "                TDM[j][i] = 1\n",
        "# 불리언 검색\n",
        "q = '대선 후보 인터뷰'\n",
        "\n",
        "rst = list()\n",
        "for wi in regexp_tokenize(q, r'\\b\\w+\\b'):\n",
        "    # TimeComplexity: Q(|Q|*avg|{d ㅌ D, if d=1}|)\n",
        "    j = CV.index(wi)\n",
        "    rst.extend(list(filter(lambda x:x==1, TDM[j])))\n",
        "2\n",
        "4\n",
        "3\n",
        "sum(map(lambda x:sum(list(filter(lambda y:y==1, x))), TDM))/len(TDM)\n",
        "1.5500758725341426\n",
        "CV.index(wi), list(filter(lambda x:x==1,TDM[7]))\n",
        "(7, [1, 1, 1])\n"
      ]
    }
  ]
}