{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuS3TR4wa9E1bVh81Ac9tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1010.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s112DwGeXeOW"
      },
      "outputs": [],
      "source": [
        "N-gram => Tokenize, LM\n",
        "1. Tokenizer => Collocation(이웃한 토큰) 패턴을 찾을 수 있음\n",
        "  => 어절(단어), 음절, 형태소, ... => 단일 단어(토큰)의 형태\n",
        "  => 단일 토큰들의 N개의 쌍 => 고려대+학교, 고려+대학교, 고려대학교, 고+려대학교, ..\n",
        "                          한성대+학교, 외국어대학교, ...\n",
        "                          ; 이름 + 학교, 대학교 => 2,3토큰\n",
        "                          ; [이름 + 학교, 대학교] => 1토큰\n",
        "2. LM\n",
        "  => P(A,B,C,D,...,Z) 못구함; 데이터X, P=0\n",
        "  => [1st, 2nd, 3rd, ...] Markov Assumption\n",
        "     Bigram; P(A,B,C,D,...,Z)=P(Z|A,...,Y)P(A,...,Y)\n",
        "                            ~=P(Z|X,Y)P(Y|X)...\n",
        "                             => P(Z,Y)/P(Y) = freq(Z,Y)/freq(Y)\n",
        "            A+B+C, A+B+D, A+B+E, ...\n",
        "            A+B + [?]\n",
        "            학+교+에(P)/에서(P*)/를/...\n",
        "            P => freq(A,B,C?, D?, E?)/ferq(A,B)\n",
        "3. Perplexity => 응집력\n",
        "   (ㅠP(i|0,...,i-1))**(1/len(i))\n",
        "     P(A,B,C,D) => P(D|A,B,C)P(A,B,C)\n",
        "    ----------- = P(D|A,B,C)\n",
        "      P(A,B,C)\n",
        "   ; P(A,B,C)/P(A,B) * P(A,B)/P(A)\n",
        "   => P(A,B,C,D)/P(A)\n",
        "      = freq(A,B,C,D)/freq(A)\n",
        "   P(고려대학교|고), P(고려대학교|고려)\n",
        "                  => 고려대 (8), 고려학원 (2), ..., 고려대학교(90)\n",
        "                     8/100     2/100           90/100\n",
        "    알잘~... => ?\n",
        "N-gram 띄어쓰기\n",
        "Cohesion Score 분절찾기\n",
        "내일;Entropy + BytePairEncoding\n",
        "내일;Normalization + Stopwords\n",
        "import re\n",
        "from os import listdir\n",
        "\n",
        "path = 'naver'\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(''.join(s[i:i+n]))\n",
        "    return rst\n",
        "\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "from collections import Counter\n",
        "\n",
        "gram1 = list()\n",
        "gram2 = list()\n",
        "gram3 = list()\n",
        "gram4 = list()\n",
        "\n",
        "for f in fileids('naver'):\n",
        "    with open(f, 'r', encoding='utf8') as fp:\n",
        "        doc = re.sub(r'\\s+', ' ', fp.read())\n",
        "        gram1.extend(ngram(doc, 1))\n",
        "        gram2.extend(ngram(doc, 2))\n",
        "        gram3.extend(ngram(doc, 3))\n",
        "        gram4.extend(ngram(doc, 4))\n",
        "\n",
        "gram1 = Counter(gram1)\n",
        "gram2 = Counter(gram2)\n",
        "gram3 = Counter(gram3)\n",
        "gram4 = Counter(gram4)\n",
        "len(gram1), len(gram2), len(gram3), len(gram4)\n",
        "(1186, 17075, 46925, 77077)\n",
        "'코스피가 이스라엘-팔레스타인 전쟁에 따른 국제정세 불안 속에서도 장초반 1% 넘게 오르고 있다.'\n",
        "data = re.sub(r'\\s', '', '코스피가 이스라엘-팔레스타인 전쟁에 따른 국제정세 불안 속에서도 장초반 1% 넘게 오르고 있다.')\n",
        "# 다음글자가 ' '인지 보는 함수\n",
        "#      => 확률적으로 가장 큰 다음글자를 return\n",
        "# N-gram(bigram;(A,B*)/(A*), trigram;(A,B,C*)/(A,B*))\n",
        "def spacing(s):\n",
        "    gram = {1:gram1, 2:gram2, 3:gram3, 4:gram4}\n",
        "    given = gram[len(s)]\n",
        "    candi = gram[len(s)+1]\n",
        "\n",
        "    c = dict(filter(lambda r:re.match(s, r[0]), candi.items()))\n",
        "\n",
        "    maxkey = ' '\n",
        "    if len(c) > 0:\n",
        "        maxkey = max(c, key=c.get)\n",
        "#     print(c)\n",
        "#     print(maxkey, c[maxkey])\n",
        "\n",
        "    return maxkey\n",
        "spacing('학교')[-1]\n",
        "' '\n",
        "N = 2\n",
        "rst = ''\n",
        "data = '한 번 저지른 일은 되돌릴 수 없다며 강하게 방류 중단을 요구하고 있습니다.'.replace(' ', '')\n",
        "for i in range(len(data)):\n",
        "    if i < N:\n",
        "        if N > 1:\n",
        "            key = rst[-N:] if len(rst)>1 else data[:i+1]\n",
        "            if spacing(key)[-1] == ' ':\n",
        "                rst += ' '\n",
        "        rst += data[i]\n",
        "    else:\n",
        "        if spacing(rst[-N:])[-1] == ' ':\n",
        "            rst += ' '\n",
        "        rst += data[i]\n",
        "rst\n",
        "' 한번 저지른 일은 되돌릴 수 없다며 강하게 방류 중단을 요구하고 있습니다.'\n",
        "# Cohesion Score => Perplexity\n",
        "image.png\n",
        "\n",
        "# 형태소+형태소+형태소 => 어절\n",
        "# 고려대는, 고려대에서는, 고려대와, 고려대로, ...\n",
        "# Perplexity(고려대에서는) = P(려|고)*P(대|고려)*P(에|고려대)*...\n",
        "#                      => (P(고려)/P(고))*(P(고려대)/P(고려))*...\n",
        "#                      => P(고려대에서는)/P(고) > P(고려대)/P(고)\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
        "\n",
        "word = list()\n",
        "for f in fileids('naver'):\n",
        "    with open(f, 'r', encoding='utf8') as fp:\n",
        "        word.extend(regexp_tokenize(re.sub(r'\\s+', ' ', fp.read()),\n",
        "                                    r'\\b\\w+\\b'))\n",
        "word = Counter(word)\n",
        "# word로부터 s로 시작하는 모든 빈도의 합 return\n",
        "def find(s):\n",
        "    return sum(dict(filter(lambda r:r[0].startswith(s),\n",
        "                    word.items())\n",
        "                   ).values())\n",
        "key = '말했다'\n",
        "before = 0.0\n",
        "for i in range(1,len(key)):\n",
        "    p = find(key[0:i+1])/find(key[0])\n",
        "    print(key[:i+1], find(key[0:i+1])/find(key[0]))\n",
        "    print('자름' if before > p else '놔둠')\n",
        "    before = p\n",
        "말했 0.5392156862745098\n",
        "놔둠\n",
        "말했다 0.5294117647058824\n",
        "자름\n",
        "# Branching Entropy\n",
        "entropy: -sum(p log p=.5)\n",
        "|\n",
        "|      *(불확실성)\n",
        "|   *    *\n",
        "-[*]--|---[*]--1*--------2\n",
        "\n",
        "말했다, 말했고, 말했는데, 말했었고, ...\n",
        "P => P(다|말했) P(고|말했) P(는|말했) P(었|말했)\n",
        "def findkey(s):\n",
        "    return list(\n",
        "                set(\n",
        "                    list(\n",
        "                    map(lambda k:k[:len(s)+1],\n",
        "                        list(dict(filter(lambda r:r[0].startswith(s),\n",
        "                                     word.items())\n",
        "                             ).keys()\n",
        "                        )))\n",
        "                )\n",
        "           )\n",
        "from math import log\n",
        "\n",
        "key = '아르헨티나'\n",
        "before = 0.0\n",
        "for i in range(len(key)):\n",
        "    total = 0.0\n",
        "    for k in findkey(key[:i+1]):\n",
        "        p = find(k)/find(key[:i+1])\n",
        "    #     print(k, -p*log(p))\n",
        "        total += -p*log(p)\n",
        "    print(key[:i+1], total, '자름' if before < total else '냅둠')\n",
        "    before = total\n",
        "아 3.010214705057504 자름\n",
        "아르 0.18490739916777568 냅둠\n",
        "아르헨 0.0 냅둠\n",
        "아르헨티 0.0 냅둠\n",
        "아르헨티나 1.0287719079404922 자름\n",
        "findkey('아르헨티나')\n",
        "['아르헨티나', '아르헨티나가', '아르헨티나로', '아르헨티나의', '아르헨티나뿐', '아르헨티나에']\n",
        "raw = '''\n",
        "low low low low low lowest lowest newer newer\n",
        "newer newer newer newer wider wider wider\n",
        "'''\n",
        "\n",
        "# 단어 = 어절\n",
        "data = dict()\n",
        "for k, v in Counter(raw.split()).items():\n",
        "    data[' '.join(tuple(k))+' </w>'] = v\n",
        "data\n",
        "{'l o w </w>': 5,\n",
        " 'l o w e s t </w>': 2,\n",
        " 'n e w e r </w>': 6,\n",
        " 'w i d e r </w>': 3}\n",
        "def findpair(d, n=2):\n",
        "    rst = dict()\n",
        "\n",
        "    for k, v in d.items():\n",
        "#         'l o w </w>' => ['l', 'o', 'w', '</w>']\n",
        "        k = k.split()\n",
        "        for i in range(len(k)-(n-1)):\n",
        "            key = ' '.join(k[i:i+n])\n",
        "            if key in rst:\n",
        "                rst[key] += v\n",
        "            else:\n",
        "                rst[key] = v\n",
        "    return rst\n",
        "keys = findpair(data)\n",
        "best = max(keys, key=keys.get)\n",
        "rst = dict()\n",
        "for k, v in data.items():\n",
        "    rst[re.sub(best, best.replace(' ', ''), k)] = v\n",
        "for _ in range(5):\n",
        "    keys = findpair(data)\n",
        "    best = max(keys, key=keys.get)\n",
        "\n",
        "    rst = dict()\n",
        "    for k, v in data.items():\n",
        "        rst[re.sub(best, best.replace(' ', ''), k)] = v\n",
        "\n",
        "    data = rst\n",
        "    print(best, rst)\n",
        "e r {'l o w </w>': 5, 'l o w e s t </w>': 2, 'n e w er </w>': 6, 'w i d er </w>': 3}\n",
        "er </w> {'l o w </w>': 5, 'l o w e s t </w>': 2, 'n e w er</w>': 6, 'w i d er</w>': 3}\n",
        "l o {'lo w </w>': 5, 'lo w e s t </w>': 2, 'n e w er</w>': 6, 'w i d er</w>': 3}\n",
        "lo w {'low </w>': 5, 'low e s t </w>': 2, 'n e w er</w>': 6, 'w i d er</w>': 3}\n",
        "n e {'low </w>': 5, 'low e s t </w>': 2, 'ne w er</w>': 6, 'w i d er</w>': 3}\n",
        "[k.split() for k in data]\n",
        "[['low', '</w>'],\n",
        " ['low', 'e', 's', 't', '</w>'],\n",
        " ['ne', 'w', 'er</w>'],\n",
        " ['w', 'i', 'd', 'er</w>']]\n",
        "data = dict()\n",
        "for k, v in word.items():\n",
        "    data[' '.join(tuple(k))] = v\n",
        "\n",
        "for _ in range(100):\n",
        "    keys = findpair(data)\n",
        "    best = max(keys, key=keys.get)\n",
        "\n",
        "    rst = dict()\n",
        "    for k, v in data.items():\n",
        "        rst[re.sub(best, best.replace(' ', ''), k)] = v\n",
        "\n",
        "    data = rst\n",
        "    print(best.replace(' ', ''))\n",
        "에서\n",
        "했다\n",
        "으로\n",
        "있다\n",
        "니다\n",
        "하고\n",
        "하는\n",
        "00\n",
        "이다\n",
        "습니다\n",
        "한다\n",
        "20\n",
        "있는\n",
        "이라\n",
        "까지\n",
        "지난\n",
        "한국\n",
        "지만\n",
        "서울\n",
        "후보\n",
        "대한\n",
        "들이\n",
        "기술\n",
        "10\n",
        "에는\n",
        "레이\n",
        "됐다\n",
        "부터\n",
        "202\n",
        "일본\n",
        "이어\n",
        "면서\n",
        "하기\n",
        "5일\n",
        "시장\n",
        "판매\n",
        "기자\n",
        "연구\n",
        "통령\n",
        "미국\n",
        "대통령\n",
        "우리\n",
        "적으로\n",
        "전기\n",
        "중국\n",
        "다는\n",
        "적인\n",
        "영화\n",
        "뉴스\n",
        "위해\n",
        "었다\n",
        "통해\n",
        "정부\n",
        "플레이\n",
        "경제\n",
        "관련\n",
        "이라고\n",
        "사진\n",
        "기업\n",
        "에게\n",
        "라고\n",
        "하게\n",
        "예술\n",
        "디스\n",
        "디스플레이\n",
        "후보자\n",
        "마트\n",
        "달러\n",
        "다고\n",
        "부산\n",
        "시간\n",
        "전자\n",
        "개발\n",
        "말했다\n",
        "이번\n",
        "하지\n",
        "민주\n",
        "19\n",
        "것으로\n",
        "된다\n",
        "밝혔\n",
        "제공\n",
        "보다\n",
        "들의\n",
        "의원\n",
        "스마트\n",
        "들어\n",
        "하며\n",
        "이날\n",
        "시스\n",
        "산업\n",
        "는데\n",
        "최근\n",
        "기를\n",
        "하다\n",
        "대표\n",
        "국제\n",
        "세계\n",
        "에도\n",
        "사람\n",
        "Tokeninzing** => 전처리 시작\n",
        "1. 문장 분리\n",
        "2. tokens 분리 => Feature Extraction/Selection\n",
        "   => 구두점 토큰화, 정규식, 트위터, ...\n",
        "      형태소, 품사, 명사 => 사전\n",
        "      stem(어간)ming, lema(표제어,어근)tization\n",
        "      Eng. Porter's s/es/ed/..,\n",
        "      N-gram\n",
        "      Kor. Perplexity, Entropy, BPE\n",
        "      WPM, SPM\n",
        "3. Normalization => Filtering/Selection\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.open(\n",
        "    stopwords.fileids()[stopwords.fileids().index('english')]).read()\n",
        "from string import punctuation\n",
        "\n",
        "s = 'To be or not to be.'\n",
        "rst = []\n",
        "for w in re.sub(f'[{re.escape(punctuation)}]', '', s.lower()).split():\n",
        "    if w not in stop:\n",
        "        rst.append(w)\n",
        "    else:\n",
        "        print(f'skipped:{w}')\n",
        "' '.join(rst)\n",
        "skipped:to\n",
        "skipped:be\n",
        "skipped:or\n",
        "skipped:not\n",
        "skipped:to\n",
        "skipped:be\n",
        "''"
      ]
    }
  ]
}