{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDX6XYRM6umXNPcWoCMfXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/1013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9_e5oxpX3nu"
      },
      "outputs": [],
      "source": [
        "Information Retrieval => Text Mining      != NLP\n",
        "                         ;비정형데이터(텍스트)    ;말의 순서(Sequence)\n",
        "=> 정보요구(Information needs)로부터 가장 연관된(Relevance) 문서 집합을 순위화(Ranking)\n",
        "\n",
        "[Concept]\n",
        "1. 정보요구=Query, representation\n",
        "2. 데이터(문서) representation\n",
        "3. 1-2의 관계를 잘 연관성(Relevance) => Search Engine / Retrieval Model\n",
        "1.2. 둘다, BoW(Independent each term)+N-Gram => Improved BoW\n",
        "\n",
        "[Components]\n",
        "1. Crawler; 데이터수집기\n",
        "2. Indexer; 텍스트전처리\n",
        "   ; Structure => Term-Doc. Matrix => Iverted Index(역색인)\n",
        "                  Doc.-Term Matrix\n",
        "                        term1 term2 ...\n",
        "                  doc1  => 책 제일 뒤 색인표\n",
        "                  doc2  (D,V)\n",
        "                  ...\n",
        "                  O(|Q|*D*|D|)\n",
        "                        doc1 doc2 ...\n",
        "                  term1 (V,D)\n",
        "                  term2\n",
        "                  ...\n",
        "                  O(|Q|*|L|)\n",
        "   ; TDM 구조 변환 => Linked List\n",
        "     List(순서;DataStructure) != Array(같은Type, 연속된)\n",
        "3. Analyzer => Representation; 데이터베이스\n",
        "4. Model\n",
        "--------------------------------------\n",
        "5. Evaluation(User Relevance Feedback)\n",
        "6. Personalized...\n",
        "import re\n",
        "from os import listdir\n",
        "\n",
        "def ngram(s, n=2):\n",
        "    rst = list()\n",
        "    for i in range(len(s)-(n-1)):\n",
        "        rst.append(''.join(s[i:i+n]))\n",
        "    return rst\n",
        "\n",
        "def fileids(path, ext='txt'):\n",
        "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
        "                       listdir(path)))\n",
        "    return list(map(lambda f:f'{path}/{f}', files))\n",
        "# 1. Matrix - 어제 (D*V*bits)\n",
        "# 2. Dictionary (D*|L|)\n",
        "# 3. List (tuple)\n",
        "# 4. InvertedIndex(File) (tuple(32bits*3))\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "# 2. Dictionary\n",
        "# TDM = [\n",
        "#       0:Term1:  [Doc1, Doc2, ...] |D|,\n",
        "#         []\n",
        "#       ]\n",
        "# TDM = {\n",
        "#         {Term1:[Doc1, Doc2, ...]} |t1ㅌD|\n",
        "#       }\n",
        "\n",
        "p1 = re.compile(r'\\s+')\n",
        "\n",
        "D = list()\n",
        "V = list()\n",
        "TDM = dict()\n",
        "\n",
        "for file in fileids('naver'):\n",
        "    i = len(D)\n",
        "    D.append(file)\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        for t in regexp_tokenize(p1.sub(' ', fp.read()), r'\\b\\w+\\b'):\n",
        "            for g in ngram(t):\n",
        "                if g not in V:\n",
        "                    V.append(g)\n",
        "                j = V.index(g)\n",
        "\n",
        "                if j not in TDM.keys():\n",
        "                    TDM[j] = list()\n",
        "\n",
        "                if i not in TDM[j]:\n",
        "                    TDM[j].append(i)\n",
        "len(TDM), len(V), len(D), len(TDM[0])\n",
        "(13241, 13241, 87, 1)\n",
        "sum([len(v) for k,v in TDM.items()])/len(V) # => |L|\n",
        "2.7234347858923043\n",
        "i2w = lambda i:V[i]\n",
        "w2i = lambda w:V.index(w)\n",
        "\n",
        "i2d = lambda i:D[i]\n",
        "d2i = lambda d:D.index(d)\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "result = list()\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            j = w2i(g)\n",
        "            result.append(TDM[j])\n",
        "# prefix, infix, postfix\n",
        "#           1 + 2\n",
        "#   + 1 2          1 2 +\n",
        "\n",
        "# OR\n",
        "# candidates = list()\n",
        "# for r in result:\n",
        "#     candidates.extend(r)\n",
        "# list(set(candidates))\n",
        "\n",
        "# AND\n",
        "candidates = list()\n",
        "for r in result:\n",
        "    candidates = list(set(candidates).intersection(r))\n",
        "candidates\n",
        "[]\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "# 3. List\n",
        "# TDM = {\n",
        "#         {Term1:[Doc1, Doc2, ...]} |t1ㅌD|\n",
        "#       }\n",
        "\n",
        "# TDM = {\n",
        "#         {Term1:Posting 몇 번째 위치}\n",
        "#       }\n",
        "# Posting = [\n",
        "#            0:(doc1, 1, next:1)\n",
        "#            1:(doc2, 1, next:-1)\n",
        "#           ]\n",
        "\n",
        "p1 = re.compile(r'\\s+')\n",
        "\n",
        "D = list()\n",
        "V = list()\n",
        "TDM = dict()\n",
        "Posting = list()\n",
        "\n",
        "for file in fileids('naver'):\n",
        "    i = len(D)\n",
        "    D.append(file)\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        for t in regexp_tokenize(p1.sub(' ', fp.read()), r'\\b\\w+\\b'):\n",
        "            for g in ngram(t):\n",
        "                if g not in V:\n",
        "                    V.append(g)\n",
        "                j = V.index(g)\n",
        "\n",
        "                if j not in TDM.keys():\n",
        "                    pos = len(Posting)\n",
        "                    Posting.append((i,1,-1))\n",
        "                    TDM[j] = pos\n",
        "                else:\n",
        "                    pos = len(Posting)\n",
        "                    Posting.append((i,1,TDM[j]))\n",
        "                    TDM[j] = pos\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "result = list()\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            result.append(list())\n",
        "\n",
        "            j = w2i(g)\n",
        "            pos = TDM[j]\n",
        "            while pos != -1:\n",
        "                i, freq, npos = Posting[pos]\n",
        "                pos = npos\n",
        "\n",
        "                result[-1].append(i)\n",
        "            result[-1] = list(set(result[-1]))\n",
        "sorted(result[0])\n",
        "[22, 28, 34, 43, 53, 72, 79, 81, 82, 84]\n",
        "# Linked List => Posting\n",
        "# on-disk\n",
        "fp = open()\n",
        "0->fp.tell()\n",
        "data\n",
        "    ^\n",
        "    위치:fp.tell()\n",
        "\n",
        "\n",
        "1주소:(다음주소:파일위치)\n",
        "        3주소:****\n",
        "   4주소:(다음주소:-1)\n",
        "            2주소:(다음주소:3)\n",
        "\n",
        "[0:1, 1:2, ...] => List\n",
        "\n",
        "class {\n",
        "    int no;\n",
        "    void* next;\n",
        "} NODE;\n",
        "\n",
        "class {\n",
        "    private public protcted NODE* head;\n",
        "    NODE* tail;\n",
        "\n",
        "    int (*push(void*));\n",
        "    int (*pop(void*));\n",
        "} LinkedList;\n",
        "\n",
        "LinkedList list;\n",
        "list->head = (NODE*)malloc(sizeof(NODE));\n",
        "memset(list->head, 0, sizeof(NODE));\n",
        "\n",
        "List().append()\n",
        "list.push = push;\n",
        "\n",
        "int push(void* a) {\n",
        "    (NODE*)a->\n",
        "}\n",
        "\n",
        "list.push()\n",
        "from struct import pack, unpack\n",
        "these can be preceded by a decimal repeat count:\n",
        "      x: pad byte (no data); c:char; b:signed byte; B:unsigned byte;\n",
        "      ?: _Bool (requires C99; if not available, char is used instead)\n",
        "      h:short; H:unsigned short; i:int; I:unsigned int;\n",
        "      l:long; L:unsigned long; f:float; d:double; e:half-float.\n",
        "    Special cases (preceding decimal count indicates length):\n",
        "      s:string (array of char); p: pascal string (with count byte).\n",
        "    Special cases (only available in native format):\n",
        "      n:ssize_t; N:size_t;\n",
        "      P:an integer type that is wide enough to hold a pointer.\n",
        "    Special case (not in native mode unless 'long long' in platform C):\n",
        "      q:long long; Q:unsigned long long\n",
        "unpack('iii', pack('iii', 1, 1, -1))\n",
        "(1, 1, -1)\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "# 4. File TDM\n",
        "# TDM = {\n",
        "#         {Term1:Posting 몇 번째 위치}\n",
        "#       }\n",
        "# Posting = [\n",
        "#            0:(doc1, 1, next:0)\n",
        "#            1:(doc2, 1, next:-1)\n",
        "#           ]\n",
        "# FILE\n",
        "# Posting = 0:(123:fp)3:(110:fp)21-1\n",
        "\n",
        "p1 = re.compile(r'\\s+')\n",
        "\n",
        "D = list()\n",
        "V = list()\n",
        "TDM = dict()\n",
        "\n",
        "Posting = open('posting.dat', 'wb')\n",
        "\n",
        "for file in fileids('naver'):\n",
        "    i = len(D)\n",
        "    D.append(file)\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        for t in regexp_tokenize(p1.sub(' ', fp.read()), r'\\b\\w+\\b'):\n",
        "            for g in ngram(t):\n",
        "                if g not in V:\n",
        "                    V.append(g)\n",
        "                j = V.index(g)\n",
        "\n",
        "                if j not in TDM.keys():\n",
        "                    pos = Posting.tell()\n",
        "                    Posting.write(pack('iii', i, 1, -1))\n",
        "                    TDM[j] = pos\n",
        "                else:\n",
        "                    pos = Posting.tell()\n",
        "                    Posting.write(pack('iii', i, 1, TDM[j]))\n",
        "                    TDM[j] = pos\n",
        "\n",
        "Posting.close()\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "result = list()\n",
        "\n",
        "Posting = open('posting.dat', 'rb')\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            result.append(list())\n",
        "\n",
        "            j = w2i(g)\n",
        "            pos = TDM[j]\n",
        "            while pos != -1:\n",
        "                Posting.seek(pos)\n",
        "                i, freq, npos = unpack('iii', Posting.read(12))\n",
        "                pos = npos\n",
        "\n",
        "                result[-1].append(i)\n",
        "            result[-1] = list(set(result[-1]))\n",
        "\n",
        "Posting.close()\n",
        "sorted(result[0])\n",
        "[22, 28, 34, 43, 53, 72, 79, 81, 82, 84]\n",
        "unpack('d', pack('d', 0.0))\n",
        "(0.0,)\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "# 4. File TDM - 빈도를 고려할 수 있도록\n",
        "# 개별 코퍼스 별로 처리\n",
        "\n",
        "p1 = re.compile(r'\\s+')\n",
        "\n",
        "D = list()\n",
        "V = list()\n",
        "Dictionary = dict()\n",
        "\n",
        "# w => 새로생성 = 0\n",
        "Posting = open('posting.dat', 'wb')\n",
        "Posting.close()\n",
        "\n",
        "# 전체 코퍼스(data)\n",
        "for file in fileids('naver'):\n",
        "    i = len(D)\n",
        "    D.append(file)\n",
        "\n",
        "    # Local\n",
        "    # 코퍼스 1개 open\n",
        "    with open(file, 'r', encoding='utf8') as fp:\n",
        "        corpus = fp.read()\n",
        "\n",
        "    localTDM = dict()\n",
        "    localPosting = open('local.dat', 'wb')\n",
        "\n",
        "    # Tokenizing+Normalizing\n",
        "    for t in regexp_tokenize(p1.sub(' ', corpus), r'\\b\\w+\\b'):\n",
        "        for g in ngram(t):\n",
        "            # 문서 1개 작업하는 중에, 단어가 처음 등장\n",
        "            if g not in localTDM.keys():\n",
        "                pos = localPosting.tell()\n",
        "                localPosting.write(pack('ii', 1, -1))\n",
        "                localTDM[g] = pos\n",
        "            else:\n",
        "                pos = localPosting.tell()\n",
        "                localPosting.write(pack('ii', 1, localTDM[g]))\n",
        "                localTDM[g] = pos\n",
        "\n",
        "    localPosting.close()\n",
        "\n",
        "    # Update; Local -> Global\n",
        "    Posting = open('posting.dat', 'ab')\n",
        "    localPosting = open('local.dat', 'rb')\n",
        "\n",
        "    # k=단어, v=파일위치\n",
        "    for k, v in localTDM.items():\n",
        "        if k not in V:\n",
        "            V.append(k)\n",
        "\n",
        "        j = V.index(k)\n",
        "\n",
        "        freq = 0\n",
        "        pos = v\n",
        "        while pos != -1:\n",
        "            localPosting.seek(pos)\n",
        "            f, npos = unpack('ii', localPosting.read(8))\n",
        "            pos = npos\n",
        "            freq += 1\n",
        "\n",
        "        if j not in Dictionary.keys():\n",
        "            pos = Posting.tell()\n",
        "            Posting.write(pack('iii', i, freq, -1))\n",
        "            Dictionary[j] = pos\n",
        "        else:\n",
        "            pos = Posting.tell()\n",
        "            Posting.write(pack('iii', i, freq, Dictionary[j]))\n",
        "            Dictionary[j] = pos\n",
        "\n",
        "    localPosting.close()\n",
        "    Posting.close()\n",
        "# Local\n",
        "LocalTDM = {'단어':'로컬파일 위치', ...}\n",
        "LocalPosting\n",
        "1다음위치1다음위치1다음위치1다음위치\n",
        "while -1:\n",
        "    freq += 1\n",
        "\n",
        "# Global\n",
        "Dictionary = {j:글로벌파일위치, ...}\n",
        "Posting\n",
        "ifreq다음위치ifreq다음위치ifreq다음위치...\n",
        "(i, freq, 다음위치)\n",
        "fp = open('posting.dat', 'wb') # a-appending 마지막위치\n",
        "print(fp.tell())\n",
        "fp.close()\n",
        "0\n",
        "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
        "\n",
        "result = list()\n",
        "\n",
        "Posting = open('posting.dat', 'rb')\n",
        "\n",
        "for t in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
        "    for g in ngram(t): # ngram => tokenizing\n",
        "        if g in V:\n",
        "            result.append(list())\n",
        "\n",
        "            j = w2i(g)\n",
        "            pos = Dictionary[j]\n",
        "            while pos != -1:\n",
        "                Posting.seek(pos)\n",
        "                i, freq, npos = unpack('iii', Posting.read(12))\n",
        "                pos = npos\n",
        "\n",
        "                result[-1].append(i)\n",
        "            result[-1] = list(set(result[-1]))\n",
        "\n",
        "Posting.close()\n",
        "P(Y|X,theta=p)\n",
        "\n",
        "P(Y) = P(Yㅌresult|X) + P(!Yㅌresult|X)\n",
        "\n",
        "(Java)Luciene <= Elastic Search(IR, Database X)\n",
        "                 => Inverted Index(Database X)\n",
        "                 Hadoop => Database X"
      ]
    }
  ]
}