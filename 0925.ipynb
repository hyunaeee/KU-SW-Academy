{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9t5iNW0N4DA+IjyhEwkwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunaeee/KU-SW-Academy/blob/main/0925.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_3-bM3V747"
      },
      "outputs": [],
      "source": [
        "from requests import request\n",
        "from requests.exceptions import HTTPError\n",
        "from time import sleep\n",
        "\n",
        "def download(url, params={}, method='GET', retries=3):\n",
        "    resp = None\n",
        "\n",
        "    try:\n",
        "        resp = request(method, url,\n",
        "                       params=params if method=='GET' else {},\n",
        "                       data=params if method=='POST' else {},\n",
        "                       headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'})\n",
        "        resp.raise_for_status()\n",
        "        # 아니면,\n",
        "        # if resp.status_code != 200:\n",
        "    except HTTPError as e:\n",
        "        if 500 <= e.response.status_code:\n",
        "            if retries > 0:\n",
        "                sleep(3)\n",
        "                resp = download(url, params=params,\n",
        "                                method=method,\n",
        "                                retries=retries-1)\n",
        "            else:\n",
        "                print('재방문 횟수 초과')\n",
        "        else:\n",
        "            print('Request', resp.request.headers)\n",
        "            print('Response', e.response.headers)\n",
        "\n",
        "    return resp\n",
        "url = 'https://pythonscraping.com/pages/javascript/ajaxDemo.html'\n",
        "resp = download(url)\n",
        "resp.headers['content-type']\n",
        "'text/html'\n",
        "from bs4 import BeautifulSoup\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "# 첫 resp/resp => DOM\n",
        "dom.select_one('#content').text.strip()\n",
        "\"This is some content that will appear on the page while it's loading. You don't care about scraping this.\"\n",
        "from requests.compat import urljoin\n",
        "\n",
        "newurl = urljoin(resp.request.url, 'loadedContent.php')\n",
        "resp = download(newurl)\n",
        "resp.headers['content-type']\n",
        "'text/html; charset=UTF-8'\n",
        "# XHR객체를 이용하여 AJAX를 통해 DHTML한 결과\n",
        "resp.text\n",
        "'Here is some important text you want to retrieve! <p/><button id=\"loadedButton\">A button to click!</button>'\n",
        "url = 'https://vsuggest.search.daum.net/v2/sushi/pc/get'\n",
        "params = {'q':'카'}\n",
        "\n",
        "resp = download(url, params)\n",
        "from requests import get\n",
        "resp.headers['content-type']\n",
        "'application/json;charset=UTF-8'\n",
        "for subkey in resp.json()['subkeys']:\n",
        "    print(subkey['keyword'])\n",
        "카카오맵\n",
        "카카오톡\n",
        "카카오\n",
        "카카오 페이지\n",
        "카카오톡 pc버전 다운로드\n",
        "카카오뱅크\n",
        "칸투칸\n",
        "카눈\n",
        "카카오 메일\n",
        "신한카드\n",
        "카나리아 바이오\n",
        "카르텔 뜻\n",
        "카카오 주가\n",
        "카니발 하이브리드\n",
        "카카오 웹툰\n",
        "# params['callback'] = 'jQuery360010040960402367771_1695601996356'\n",
        "resp = download(url, params)\n",
        "resp.headers['content-type']\n",
        "'application/javascript'\n",
        "del params['callback']\n",
        "while True:\n",
        "    q = input()\n",
        "\n",
        "    if q == '종료':\n",
        "        break\n",
        "\n",
        "    params['q'] = q\n",
        "    resp = download(url, params)\n",
        "    print(', '.join([subkey['keyword']\n",
        "                     for subkey in resp.json()['subkeys']]))\n",
        "카\n",
        "카카오맵, 카카오톡, 카카오, 카카오 페이지, 카카오톡 pc버전 다운로드, 카카오뱅크, 칸투칸, 카눈, 카카오 메일, 카르텔 뜻, 신한카드, 카나리아 바이오, 카카오 주가, 카니발, 카니발 하이브리드\n",
        "카리\n",
        "카리나, 카리토 포텐, 카리토포텐 가격, 카리스, 카리토 포텐 효능, 카링킷 프로2, 카링킷, 카린, 카리브해 위치, 카리나 언니, 카리브해, 카림 벤제마, 카리스마, 카리스 가드레일, 카린 선글라스\n",
        "카리나\n",
        "카리나, 카리나 언니, 카리나 키, 카리나 고향, 에스파 카리나, 카리나 미드, 카리나 인스타, 카리나 사주, 카리나 윈터, 카리나 나이, 카리나 고등학교, 카리나 영어, 카리나 뜻, 카리나 프로필, 카리나 생일\n",
        "종료\n",
        "url = 'https://brunch.co.kr/search'\n",
        "params = {\n",
        "    'q':'카리나',\n",
        "    'type':'article'\n",
        "}\n",
        "\n",
        "resp = download(url, params)\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "import re\n",
        "dom.find_all(text=re.compile('카리나'))\n",
        "[]\n",
        "dom.select_one('#resultArticle').contents\n",
        "[]\n",
        "url = 'https://api.brunch.co.kr/v1/search/article'\n",
        "params = {\n",
        "    'q':'카리나',\n",
        "    'page':'1',\n",
        "    'pageSize':'20',\n",
        "    'highlighter':'n',\n",
        "    'escape':'y',\n",
        "    'sortBy':'accu'\n",
        "}\n",
        "resp = download(url, params)\n",
        "resp.headers['content-type']\n",
        "'application/json;charset=UTF-8'\n",
        "for item in resp.json()['data']['list']:\n",
        "    print(item['title'])\n",
        "시에나를 제대로 즐기는 방법 - 만지아의 탑, 캄포광장,시에나 대성당, 카타리나 그리고 카리나\n",
        "카리나가 선택한 공항패션 - Weekly Celeb\n",
        "카리나 3행시 할게요 #에스파 #aespa\n",
        "카리나 3행시 할게요 #에스파 #aespa\n",
        "카리나와 차은우로 태교를 하고 있어요. - 21. 엄마가 기분 좋으면 그게 태교라는 아내\n",
        "화제에 오르고 있는 육상계 카리나 ○○○ 선수는? - 에스파 카리나 닮은 꼴 육상 선수\n",
        "의결권을 위임하면 카리나 사인을 준다고요? - SM 엔터테인먼트의 주주총회, 어떤 법적 구설수가 있었나\n",
        "트리스탄이며 파르지팔인 그는 누구인가? - 단눈치오와 토스티의 &lt;아마란타의 네 개의 노래&gt;\n",
        "짐승이 되어가고 있는 것 같다 - D+14 포르투갈길 14일 차\n",
        "[고다르 시네마][2022] - 제 79회 베니스 국제 영화제 비경쟁부문\n",
        "24. 외모의 중요성과 활개치는 외모지상주의\n",
        "&#39;차쥐뿔&#39; 이영지가 주는 특별한 위로 - [문제적 여자들] &lt;차린건 쥐뿔도 없지만&gt; 호스트 이영지의 진정성\n",
        "제법 뚠뚠한 일상. 4화 - 4. 짜증 난다의 의미\n",
        "[트렌드 언박싱] 여름에도 내추럴한 멋을 포기 못한다면 - 2023년 5월의 패션, 뷰티, 라이프 트렌드\n",
        "솔직함, 치부를 드러내는 강함에 대하여 - 고집쟁이의 영화추천 (10) : 미치광이 삐에로 리뷰\n",
        "성형수술과 아이덴티티 - 시뮬레이션으로 알아본 원본 상실 순간\n",
        "영화: 캐러비언의 해적 5-죽은 자는 말이 없다 - 저주받은 아버지 윌 터너를 찾아 나선 아들 헨리 터너\n",
        "[IP 비즈니스] ③ &#39;광야&#39;로 향하는 엔터테인먼트\n",
        "광기; 미쳐 가는 세상의 한 가운데 - 미친 세상에서 미치지 않는 법\n",
        "url = 'https://comic.naver.com'\n",
        "resp = download(url)\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "# 1. 웹툰 목록 페이지\n",
        "url = 'https://comic.naver.com/api/home/component?type=DAILY_WEBTOON'\n",
        "resp = download(url)\n",
        "resp.headers\n",
        "{'Content-Type': 'application/json', 'x-content-type-options': 'nosniff', 'x-xss-protection': '1; mode=block', 'Cache-Control': 'no-cache, no-store, max-age=0, must-revalidate', 'Pragma': 'no-cache', 'Expires': '0', 'x-frame-options': 'SAMEORIGIN', 'Content-Encoding': 'gzip', 'referrer-policy': 'unsafe-url', 'Server': 'nfront', 'Date': 'Mon, 25 Sep 2023 01:29:42 GMT', 'Content-Length': '8381', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'Set-Cookie': 'XSRF-TOKEN=7286d156-4520-43b5-9259-4739e4ddd726; Path=/'}\n",
        "resp.json()['titleList'][0]\n",
        "{'titleId': 817019,\n",
        " 'titleName': '날 먹는 건 금지양!',\n",
        " 'thumbnailUrl': 'https://image-comic.pstatic.net/webtoon/817019/thumbnail/thumbnail_IMAG21_6845792d-40db-4cd9-ab7a-9b86995f3383.jpg',\n",
        " 'thumbnailBadgeList': ['NEW'],\n",
        " 'author': {'writers': [{'id': 346353, 'name': '조9'}],\n",
        "  'painters': [{'id': 346353, 'name': '조9'}],\n",
        "  'originAuthors': []},\n",
        " 'displayAuthor': '조9',\n",
        " 'up': True,\n",
        " 'rest': False,\n",
        " 'openToday': True}\n",
        "# 2. 특정 웹툰의 회차목록\n",
        "url = 'https://comic.naver.com/webtoon/list?titleId=812354'\n",
        "resp = download(url)\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "dom.body\n",
        "<body>\n",
        "<div id=\"root\"></div>\n",
        "</body>\n",
        "# 3. 웹툰 해당회차 이미지 목록\n",
        "url = 'https://comic.naver.com/api/article/list?titleId=812354&page=1'\n",
        "resp = download(url)\n",
        "resp.headers['content-type']\n",
        "'application/json'\n",
        "resp.json()['articleList'][0]\n",
        "{'no': 27,\n",
        " 'thumbnailUrl': 'https://image-comic.pstatic.net/webtoon/812354/27/thumbnail_202x120_c28b3105-3956-435f-b684-8f036c3725a0.jpg',\n",
        " 'subtitle': '27화 결혼식(3)',\n",
        " 'starScore': 9.96479,\n",
        " 'bgm': False,\n",
        " 'up': False,\n",
        " 'charge': False,\n",
        " 'serviceDateDescription': '23.09.23',\n",
        " 'volumeNo': 27,\n",
        " 'hasReadLog': False,\n",
        " 'recentlyReadLog': False,\n",
        " 'thumbnailClock': False,\n",
        " 'thumbnailLock': False}\n",
        "url = 'https://comic.naver.com/webtoon/detail?titleId=812354&no=27&week=thu'\n",
        "resp = download(url)\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "# dom.body.select('img')\n",
        "from requests.compat import urljoin, urlparse, urlunparse, urlencode\n",
        "\n",
        "url = 'https://comic.naver.com/api/home/component'\n",
        "params = {'type':'DAILY_WEBTOON'}\n",
        "\n",
        "URLs = []\n",
        "URLs.append((url, params))\n",
        "\n",
        "seens = []\n",
        "domain = []\n",
        "\n",
        "while URLs:\n",
        "    seed = URLs.pop(0)\n",
        "\n",
        "    resp = download(*seed)\n",
        "    seens.append(resp.request.url)\n",
        "\n",
        "    if resp.status_code != 200:\n",
        "        continue\n",
        "\n",
        "    # 3. 웹툰 해당 회차의 이미지 목록\n",
        "    if re.search('text/html', resp.headers['content-type']):\n",
        "        dom = BeautifulSoup(resp.text, 'html5lib')\n",
        "        for link in dom.select('#sectionContWide img[src]'):\n",
        "            href = link.attrs['src']\n",
        "            newurl = urljoin(resp.request.url, href)\n",
        "\n",
        "            urlc = tuple(newurl.split('?'))\n",
        "\n",
        "            if newurl not in seens and\\\n",
        "               urlc not in URLs:\n",
        "                URLs.append(urlc)\n",
        "\n",
        "    elif re.search('image/(?:(?:jpeg)|(?:gif)|(?:png))',\n",
        "                   resp.headers['content-type']):\n",
        "        fname = re.sub('[?]', '', resp.request.url.split('/')[-1])\n",
        "        with open(f'./webtoon/{fname}', 'wb') as fp:\n",
        "            fp.write(resp.content)\n",
        "\n",
        "    elif re.search('application/json', resp.headers['content-type']):\n",
        "        result = resp.json()\n",
        "\n",
        "        # 1번\n",
        "        if 'titleList' in result.keys():\n",
        "            # 2번\n",
        "            baseurl = 'https://comic.naver.com/api/article/list?titleId='\n",
        "            for newurl in [baseurl+str(r['titleId'])\n",
        "                           for r in result['titleList']][:1]:\n",
        "                urlc = tuple(newurl.split('?'))\n",
        "\n",
        "                if newurl not in seens and\\\n",
        "                   urlc not in URLs:\n",
        "                    URLs.append(urlc)\n",
        "\n",
        "        # 3번\n",
        "        elif 'articleList' in result.keys():\n",
        "            baseurl = 'https://comic.naver.com/webtoon/detail?'\n",
        "            for newurl in [baseurl+seed[-1]+'&no='+str(r['no'])\n",
        "                           for r in result['articleList']][:1]:\n",
        "                urlc = tuple(newurl.split('?'))\n",
        "\n",
        "                if newurl not in seens and\\\n",
        "                   urlc not in URLs:\n",
        "                    URLs.append(urlc)\n",
        "len(seens), len(URLs)\n",
        "(108, 0)\n",
        "import os\n",
        "os.mkdir('./webtoon')\n",
        "os.listdir('.')\n",
        "['.DS_Store',\n",
        " '0906.ipynb',\n",
        " '0925.ipynb',\n",
        " '0918.ipynb',\n",
        " 'sns.db',\n",
        " '0907.ipynb',\n",
        " '0905.ipynb',\n",
        " 'webtoon',\n",
        " '0912.ipynb',\n",
        " 'news',\n",
        " '.ipynb_checkpoints',\n",
        " '0913.ipynb',\n",
        " '0908.ipynb',\n",
        " '0911.ipynb',\n",
        " '0915.ipynb',\n",
        " '0905.db']\n",
        "# Cookies[Client] / Sessions[Server]\n",
        "Req-Resp : 로그인\n",
        ".        : 메일함\n",
        ".        : 메일쓰고\n",
        ".\n",
        "url = 'https://pythonscraping.com/pages/cookies/login.html'\n",
        "resp = download(url)\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "dom.select_one('form').attrs\n",
        "{'method': 'post', 'action': 'welcome.php'}\n",
        "for tag in dom.select('form > input[name]'):\n",
        "    print(tag.attrs)\n",
        "{'type': 'text', 'name': 'username'}\n",
        "{'type': 'password', 'name': 'password'}\n",
        "urljoin(resp.request.url, dom.select_one('form').attrs['action'])\n",
        "'https://pythonscraping.com/pages/cookies/welcome.php'\n",
        "params = list()\n",
        "for tag in dom.select('form > input[name]'):\n",
        "    params.append(tag.attrs['name']+'='+'')\n",
        "'&'.join(params)\n",
        "'username=&password='\n",
        "dom.select_one('form').attrs['method']\n",
        "'post'\n",
        "newurl = urljoin(resp.request.url, dom.select_one('form').attrs['action'])\n",
        "\n",
        "resp = download(newurl, {'username':'아무거나', 'password':'password'},\n",
        "                'POST')\n",
        "resp.status_code\n",
        "200\n",
        "resp.headers\n",
        "{'Server': 'nginx', 'Date': 'Mon, 25 Sep 2023 02:41:15 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Content-Length': '123', 'Connection': 'keep-alive', 'X-Powered-By': 'PHP/7.4.33, PleskLin', 'Set-Cookie': 'loggedin=1, username=%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip'}\n",
        "resp.text\n",
        "'\\n<h2>Welcome to the Website!</h2>\\nYou have logged in successfully! <br><a href=\"profile.php\">Check out your profile!</a>'\n",
        "resp.request.headers, resp.request.method, resp.request.url, \\\n",
        "resp.request.body\n",
        "({'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '63', 'Content-Type': 'application/x-www-form-urlencoded'},\n",
        " 'POST',\n",
        " 'https://pythonscraping.com/pages/cookies/welcome.php',\n",
        " 'username=%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98&password=password')\n",
        "resp = download(urljoin(newurl, 'profile.php'))\n",
        "resp.text\n",
        "'You\\'re not logged into the site!<br>Visit <a href=\"login.html\">the login page</a> to log in'\n",
        "request()\n",
        "from requests.cookies import cookiejar_from_dict\n",
        "cookiejar_from_dict({'username':'어쩌고'})\n",
        "<RequestsCookieJar[Cookie(version=0, name='username', value='어쩌고', port=None, port_specified=False, domain='', domain_specified=False, domain_initial_dot=False, path='/', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False)]>\n",
        "cookie = {'loggedin':'1',\n",
        "          'username':'%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98'}\n",
        "resp = request('GET', urljoin(newurl, 'profile.php'), cookies=cookie)\n",
        "resp.text\n",
        "\"Hey 아무거나! Looks like you're still logged into the site!\"\n",
        "cookie['loggedin'] = 'q345342'\n",
        "resp = request('GET', urljoin(newurl, 'profile.php'), cookies=cookie)\n",
        "resp.text\n",
        "\"Hey 아무거나! Looks like you're still logged into the site!\"\n",
        "from requests.sessions import Session\n",
        "sess = Session()\n",
        "sess.cookies.set('loggedin', '1')\n",
        "Cookie(version=0, name='loggedin', value='1', port=None, port_specified=False, domain='', domain_specified=False, domain_initial_dot=False, path='/', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False)\n",
        "from requests.compat import quote\n",
        "sess.cookies.set('username', quote('한글'))\n",
        "Cookie(version=0, name='username', value='%ED%95%9C%EA%B8%80', port=None, port_specified=False, domain='', domain_specified=False, domain_initial_dot=False, path='/', path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False)\n",
        "sess.cookies.clear()\n",
        "resp = sess.get(urljoin(newurl, 'profile.php'))\n",
        "resp.request.headers\n",
        "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'loggedin=1; username=%ED%95%9C%EA%B8%80'}\n",
        "sess.cookies.clear()\n",
        "sess.cookies\n",
        "<RequestsCookieJar[]>\n",
        "newurl = 'https://pythonscraping.com/pages/cookies/welcome.php'\n",
        "\n",
        "resp = sess.post(newurl, {'username':'아무거나', 'password':'password'})\n",
        "resp.headers\n",
        "{'Server': 'nginx', 'Date': 'Mon, 25 Sep 2023 02:57:16 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Content-Length': '123', 'Connection': 'keep-alive', 'X-Powered-By': 'PHP/7.4.33, PleskLin', 'Set-Cookie': 'loggedin=1, username=%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip'}\n",
        "sess.cookies\n",
        "<RequestsCookieJar[Cookie(version=0, name='loggedin', value='1', port=None, port_specified=False, domain='pythonscraping.com', domain_specified=False, domain_initial_dot=False, path='/pages/cookies', path_specified=False, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={}, rfc2109=False), Cookie(version=0, name='username', value='%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98', port=None, port_specified=False, domain='pythonscraping.com', domain_specified=False, domain_initial_dot=False, path='/pages/cookies', path_specified=False, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={}, rfc2109=False)]>\n",
        "resp = sess.get(urljoin(newurl, 'profile.php'))\n",
        "resp.text, resp.request.headers\n",
        "(\"Hey 아무거나! Looks like you're still logged into the site!\",\n",
        " {'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'loggedin=1; username=%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98'})\n",
        "sess.cookies.clear()\n",
        "c = '''\n",
        "'''\n",
        "\n",
        "cookies = dict()\n",
        "\n",
        "for l in c.splitlines():\n",
        "    if len(l) > 0:\n",
        "        cookies[l.split('\\t')[0]] = l.split('\\t')[1]\n",
        "for k, v in cookies.items():\n",
        "    sess.cookies.set(k,v)\n",
        "from requests import get\n",
        "resp = get('https://lms.sunde41.net')\n",
        "resp.request.headers\n",
        "{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
        "resp = sess.get('https://lms.sunde41.net')\n",
        "resp.request.headers\n",
        "resp = get('https://lms.sunde41.net', cookies=sess.cookies)\n",
        "resp.text\n",
        "# 원래 로그인 절차\n",
        "resp = get('https://lms.sunde41.net')\n",
        "dom = BeautifulSoup(resp.text, 'lxml')\n",
        "form = dom.select_one('form')\n",
        "inputs = dom.select('form input[name]')\n",
        "form.attrs, urljoin(resp.request.url, form.attrs['action'])\n",
        "({'action': '/auth/login',\n",
        "  'method': 'POST',\n",
        "  'name': 'login_user_form',\n",
        "  'class': ['m-form']},\n",
        " 'https://lms.sunde41.net/auth/login')\n",
        "for tag in inputs:\n",
        "    print(tag.attrs['name']+'='+(tag.attrs['value']\n",
        "                                 if tag.has_attr('value')\n",
        "                                 else ''))\n",
        "next=/\n",
        "email=\n",
        "password=\n",
        "remember=\n",
        "# LMS 로그인 -> 쿠키 활용\n",
        "# 수업게시판\n",
        "# 첨부자료가 있는 수업 목록만 추출\n",
        "# 첨부자료 링크로 추출"
      ]
    }
  ]
}